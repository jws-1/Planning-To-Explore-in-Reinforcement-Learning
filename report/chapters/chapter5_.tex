\chapter{Discussion}
\label{chapter5}

\section{Conclusion}
Exploration is a very important topic in RL, in tasks of interest its particularly important to do exploration \texit{well}; in terms of learning time and cost. Model-free exploration methods are widely used, although tend to be inefficient. An alternative is model-based exploration which often uses optimism or intrinsic motivation. Optimistic methods tend to be over-optimistic while intrinsically motivated methods rely on expensive computation. Furthermore, in most cases models are learned entirely from scratch and are assumed to eventually become correct. 
\\We proposed an approach to exploration that leverages an initial model, optimism and intrinsic motivation; whilst not relying on the model eventually becoming correct. This approach used Meta Actions, which are the main contribution of this work, which when given to a Planner enable it to hypothesise changes to the model. A framework was developed that utilises a Planner equipped with these Meta Actions to drive exploration; this framework was encapsulated in multiple implementations - the must successful of which was the RL VI Meta agent. With our empirical evaluation we showed that the use of Meta Actions can be useful in exploration, mostly under the condition of embedded reasonability, and allow our framework to overcome model inaccuracies, where it wouldn't be able to do so without; moreover, we showed that leveraging planning to drive exploration is much more sample efficient than ubiquitous model-free exploration methods such as $\epsilon$-greedy.
\\Meta Actions can be useful during exploration. We hope that this work opens the door for future research to fully understand how we can best use and learn Meta Actions.
% We applied this concept to reinforcement learning, by developing a framework that uses a planner, equipped with Meta Actions, to perform exploration. With our empirical evaluation we showed that the use of Meta Actions can be useful in exploration, and allow a model-based RL agent to overcome model inaccuracies, where it wouldn't be able to do so without; moreover, we showed that leveraging planning to drive exploration is much more sample efficient than ubiquitous model-free exploration methods such as $\epsilon$-greedy.

% both those that are embedded and those that are learned, can be useful in exploration, and allow a model-based RL agent to overcome 
% \\We conclude that Meta Actions are indeed useful, and in-fact are quite powerful in model-based exploration, and aid in overcoming model inaccuracies which allows the agent to leverage human knowledge embedded in an initial model. We believe that Meta Actions and their uses in exploration within RL is an interesting line of research, that we are interested in exploring further.
% \paragraph*{Meta Actions}
% \paragraph*{The Framework}
% \paragraph*{Evaluation}
\section{Limitations and Future Work}
The limitations of this work are mostly due to assumptions that were made to enable simplifications. Within this section we discuss these assumptions, alongside other limitations, and potential solutions as well as general ideas for future work.
% The limitations of this work are mostly due to assumptions or simplifications that were made to reduce the number of considerations needed to develop a framework that showcased our main contribution; the use of Meta Actions. Most notably, we assumed discrete (or discretised) state and action spaces, a deterministic reward function, full observability and episodic tasks. Within this section we discuss how we can reduce these assumptions along with other limitations and general ideas for future work.

\paragraph*{Stochastic Rewards and Bandits}
We assumed a deterministic reward function. This assumption is one that certainly does not hold in all domains, most notably within Bandit scenarios, where there is a single state and multiple actions \cite{lattimore}. Stochastic Rewards could be considered by extending model-learning to learn a \textit{tabular maximum likelihood model} for the reward function alongside the transition function.
\\Currently, the Meta Actions available regarding the rewards simply enable the reward to be increased. This could be modified to increase the probability of such rewards. Furthermore, in the Bandit setting, our approach of choosing when to call Meta Actions, by considering which would most benefit the planner, would probably not work; due to the single state nature, every change would most benefit the planner. Therefore in this case, a better way of choosing when to call Meta Actions needs to be considered - this could be through information theory, such as minimising uncertainty, or perhaps a count-based approach.

\paragraph*{Planning}
A key benefit of using an A* planner is that it is fast, at the cost of having to design a good, admissible, heuristic, and determinising the domain; our approach to stochasticity described in Section \ref{sec:342} was unsuccessful.  In many domains, designing a good heuristic by-hand is very difficult. However recent works, such as \cite{DBLP:journals/corr/abs-2107-02603}, have shown that heuristics can be learned directly - which is a potentially interesting extension to this work, which would allow a very simple planner to be used in perhaps a wide range of domains. However, the loss of accuracy due to determinisation could outweigh the value of decreasing computation.
\\Whilst Value Iteration natuarally deals with stochasticity, is generally slow due to its exhaustive nature, and as state spaces grow it may become very inefficient.
Furthermore, as state spaces grow, Value Iteration may become inefficient. Therefore, alternate, faster, planning algorithms, that work under stochasticity, could also be considered such as Upper Confidence Trees (UCT) \cite{10.1007/11871842_29}; which is the UCB algorithm \cite{auer2002finite} applied to tree search. 

\paragraph*{Continous State and Action Spaces}\label{p:cont}
We assumed discrete domains, or domains that offered discretisation. However, discretisation might not be sufficient; it's difficult to find a balance between coarse and fine grain state and action boundaries that maintain accuracy and efficiency. Thus, it's likely that function approximation based approaches would be a better fit for modelling. However, this introduces new issues; planning in continuous MDPs can be difficult and computationally expensive, cFVI  \cite{210504682} could be a potential option and UCT has also been extended to work in continuous state and action spaces \cite{10.1007/978-3-642-25566-3_32}.



\paragraph*{Partial Observability}
We assumed fully observable domains which is not always possible, particularly in real-life tasks of interests, such as those the robotics domain. Therefore, our approach could be extended to POMDPs, where planning would take place in belief space, rather than state space. Exactly solving POMDPs is an intractible problem, however various approaches have been suggested for approximate planning in POMDPs, such as POMCP \cite{NIPS2010_edfbe1af} and POMCPOW \cite{DBLP:journals/corr/abs-1709-06196}, the latter of which works under continuous belief and action spaces.

\paragraph*{Learning Meta Actions}
The method of learning Meta Actions that we proposed was rather simple and it wasn't vert effective during our experiments. However, we believe that learning Meta Actions is much more powerful than embedding them. An alternative approach to learning Meta Actions could leverage a Neural Network, trained through the replay buffer at the end of each episode, that takes that takes as input a state, and suggests modifications to the transitions and rewards relating to that state.

\paragraph*{Definition of Feasibility}
Whilst the definition of feasibility, given in Section \ref{sec:31}, is sufficient to prevent infinite hypothesising, through only allowing Meta Actions to be called once on a state-action pair, a stronger emphasis could be put on preventing contradictions to be made (particularly in the stochastic case, where we only consider the observations pertaining to the current episode or previous $N$ episodes). A formidable approach could be to follow the idea of \texit{known} and \texit{unknown} states, present in $E^3$ \cite{Kearns+Singh:2002} and R-MAX \cite{10.1162/153244303765208377}, only allowing Meta Actions to be called on \textit{unknown} states/state-action pairs; however this would introduce a hyperparameter, $m$, to define after how many observations a state/state-action pair becomes known.
\paragraph*{Task-Agnostic Exploration}

Our exploration approaches are goal-conditioned and task-specific. A useful line of investigation might be to consider task-agnostic exploration, where we explore the state space independent of any task, and then afterwards use extrinsic reward to adapt to downstream tasks, as in Plan2Explore \cite{plan2explore}. This could be done by choosing goals to plan and explore towards through intrinsic motivation, for instance seeking novel states that have high levels of uncertainty associated with them or seeking states with a long planning horizon. This could lead to generalisation to a variety of tasks in a given domain with minimal learning beyond the initial exploratory phase; this would take place in the model-free phase that we outlined within our framework.

\paragraph*{Further Benchmarking}
We restricted our benchmarking to gridworld-like domains. Whilst this enabled us to clearly evaluate and show the usefulness of using Meta Actions, it would have been worthwhile to benchmark our implementations in a wider range of domains, such as classic control tasks; this would've provided us with a more in-depth empirical evaluation, further showing the usefulness of our framework. Moreover, we only considered discrete tasks, it would be interesting to see how our framework performs in continuous domains (discretised, or with the modifications described pertaining to continous state and action spaces). These benchmarks could be done using OpenAI Gym \cite{1606.01540} and bsuite domains \cite{osband2020bsuite}.

\paragraph*{Beyond Episodic Tasks}
We only considered tasks with a finite time horizon associated with them. An interesting further line of work could be to consider tasks that have an infinite time horizon: continuous tasks. Task-agnostic exploration could naturally enable continuous tasks to be learned.

\paragraph*{Theoretical Analysis}
A theoretical analysis of Meta Actions would be very beneficial. In particular, we would like to prove that if there is a model inaccuracy, it will be discovered.

% \paragraph*{Continuous State and Action Spaces}
% We assumed discrete domains, or domains that offered discretisation. However, discretisation might not be sufficient; it's difficult to find a balance between coarse and fine grain state and action boundaries that maintain accuracy and efficiency. Thus, it's likely that function approximation based approaches would be a better fit, such as using Fitted Value Iteration as in \cite{SARA07-jong}.
% \paragraph*{Stochastic Rewards and Bandit Algorithms}
% Within this work, we made an assumption that rewards are deterministic. This assumption is one that certainly does not hold in most domains, most notably within Bandit scenarios, where there is a single state and multiple actions \cite{lattimore}. Stochastic Rewards could be considered by extending model-learning to learn a \textit{tabular maximum likelihood model} for the reward function alongside the transition function.
% \\Currently, the Meta Actions available regarding the rewards simply enable the reward to be increased. This could be modified to increase the probability of such rewards. Furthermore, in the Bandit setting, our approach of choosing when to call Meta Actions, by considering which would most benefit the planner, would probably not work; due to the single state nature, every change would most benefit the planner. Therefore in this case, a better way of choosing when to call Meta Actions needs to be considered - this could be through information theory, such as minimising uncertainty, or perhaps a count-based approach.
% \paragraph*{Partial Observability}
% We assumed fully observable domains which is not always possible, particularly in real-life tasks of interests, such as those the robotics domain. Therefore, our approach could be extended to POMDPs, where planning would take place in belief space, rather than state space. Exactly solving POMDPs is an intractible problem, however various approaches have been suggested for approximate planning in POMDPs, such as POMCP \cite{NIPS2010_edfbe1af}.
% \paragraph*{Planning}
% A key benefit of using an A* planner is that it is fast, at the cost of having to design a good, admissible, heuristic, and probably having to determinise the domain, although the approach to stochasticity described in Section \ref{sec:342} did work. 
% In many domains, designing a good heuristic by-hand is very difficult. However recent works, such as \cite{DBLP:journals/corr/abs-2107-02603}, have shown that heuristics for A* can be learned directly - which is a potentially interesting extension to this work, which would allow a very simple planner to be used in perhaps a wide range of domains. Furthermore, as state spaces grow, Value Iteration may become inefficient. Therefore, alternate, faster, planning algorithms could also be considered such as Upper Confidence Trees (UCT) \cite{10.1007/11871842_29}; which is the UCB algorithm \cite{auer2002finite} applied to tree search.
% \paragraph*{Task-Agnostic Exploration}
% Our exploration approaches are goal-conditioned and task-specific. A useful line of investigation might be to consider task-agnostic exploration, where we explore the state space independent of any task, and then afterwards use extrinsic reward to adapt to downstream tasks, as in Plan2Explore \cite{plan2explore}. This could be done by choosing goals to plan and explore towards through intrinsic motivation, for instance seeking novel states that have high levels of uncertainty associated with them or seeking states with a long planning horizon. This could lead to generalisation to a variety of tasks in a given domain with minimal learning beyond the initial exploratory phase; this would take place in the model-free phase that we outlined within our framework.
% \paragraph*{Beyond Episodic Tasks}
% We only considered tasks with a finite time horizon associated with them. An interesting further line of work could be to consider tasks that have an infinite time horizon: continuous tasks. Task-agnostic exploration could naturally enable continuous tasks to be learned.
% \paragraph*{Learning Meta Actions}
% The method of learning Meta Actions that we proposed is rather simple, and may not actually be that effective - a better solution might be missed out on, as a discrepancy was not experienced, and thus Meta Action learned, that allowed the it to be discovered. For example, we may learn a Meta Action that prevents a transition to take place (due to some obstacle, for instance), but we might not learn the inverse that enables us to hypothesise that an obstacle is not in-fact there, and a transition is possible. As an alternative, we may not directly learn Meta Actions but employ an "Operator" trained on experience in an environment, that takes as input a state, and suggests modifications to the transitions and rewards relating to that state.
% \paragraph*{Definition of Feasibility}
% Whilst the definition of feasibility, given in Section \ref{sec:31}, is sufficient to prevent infinite hypothesising, through only allowing Meta Actions to be called once on a SAS-triple, a stronger emphasis could be put on preventing contradictions to be made (particularly in the stochastic case, where we only consider the observations pertaining to the current episode). A formidable approach could be to follow the idea of \texit{known, unknown} and \textit{unvisited} states, present in $E^3$ \cite{Kearns+Singh:2002} and R-MAX \cite{10.1162/153244303765208377}, only allowing Meta Actions to be called on \textit{unknown} and \textit{unvisited} states/state-action pairs; however this would introduce a hyperparameter, $m$, to define after how many observations a state/state-action pair becomes known.