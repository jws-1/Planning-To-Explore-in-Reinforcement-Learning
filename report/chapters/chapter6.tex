\chapter{Evaluation / Discussion}
\label{chapter6}
\section{Sample Efficiency}
Sample Efficiency refers 
\subsection{Simulated}
\subsection{Real}
\begin{itemize}
    \item Sample efficiency (simulated, real, exploratory).
    \item Complexity analysis?
\end{itemize}
\section{Complexity and Efficiency}
\section{Limitations}
The main limitation of the implementations proposed of our framework relate to its scalibility, due to the tabular approaches followed for the model and the Q function as well as assumptions made regarding discretisation. Furthermore, our approach assumes full observability, which may not be the case in some real world tasks. Also, our approach does not consider stochastic rewards; it assumes that the reward received for every transition is the same, which is certainly not always the case, particularly in the case of bandit algorithms \citep{lattimore}. Our approach also only considers episodic taks, rather than continuous ones - this limits the application of our algorithm.
\section{Conclusions}
We proposed a framework that 
\section{Ideas for future work}

\subsection{Proof of Convergence, Optimality, Efficiency, etc.}

\subsection{Stochastic Rewards and Bandits}
Within this work, we made an assumption that rewards are deterministic. This assumption is one that certainly does not hold in most domains, most notably within Bandit scenarios, where there is a single state and multiple actions \citep{lattimore}. Stochastic Rewards could be considered my extending model-learning to learn a \textit{tabular maximum likelihood model} for the reward function alongside the transition function. Currently, the Meta Actions available regarding the rewards can increase reward to an observed reward, or to the maximum reward according in the model. This could be modified to increase the probability of such rewards. Furthermore, in the Bandit setting, our approach of choosing when to call Meta Actions, by considering which would most benefit the planner, would probably not work; due to the single state nature, every change would most benefit the planner. Therefore in this case, a better way of choosing when to call Meta Actions needs to be considered - this could be through information theory or perhaps a count-based approach.
\subsection{Continuous Domains}
Whilst we were able to benchmark against continuous domains by manually discretising them, this is not a very robust approach, and lots of accuracy might be lost through discretisation, especially when it's done by hand. Therefore, to scale to continous domains, the model could be approximated through function approximation methods.
\subsection{Partial Observability}
Partial Observability (through POMDPs) was not considered in this work; we assumed full observability. In real-life scenarios, especially those in the robotics domain, full observability is unrealistic. Therefore, a possible extension to this work could be to apply it to POMDPs.
\subsection{Beyond Episodic Tasks}
\subsection{Planning}
A key benefit of using an A* planner is that it is fast, at the cost of having to design a good, admissible, heuristic, and probably having to determinise the domain, although the approach to stochasticity described in Section \ref{sec:342} did work. In many domains, designing a good heuristic by-hand is very difficult, hence why we limited our evaluation of the RL-A* family of agents. However, recent works \cite{DBLP:journals/corr/abs-2107-02603}, have shown that heuristics for A* can be learned directly - which is a potentially interesting extension to this work, which would allow a very simple planner to be used.
\\Value Iteration is quite inefficient, due to its exhaustive nature. Therefore, alternate, faster, planning algorithms could be considered such as Upper Confidence Trees (UCT) \cite{10.1007/11871842_29}; which is the UCB algorithm \cite{auer2002finite} applied to tree search.
\subsection{Definition of Feasibility}
Whilst the definition of feasibility, given in Section \ref{sec:31}, is sufficient to prevent infinite hypothesising, through only allowing Meta Actions to be called once on a SAS-triple, a stronger emphasis could be put on preventing contradictions to be made (particularly in the stochastic case, where we only consider the observations pertaining to the current episode). A formidable approach could be to follow the idea of \texit{known, unknown} and \textit{unvisited} states, present in $E^3$ \cite{Kearns+Singh:2002} and R-MAX \cite{10.1162/153244303765208377}, only allowing Meta Actions to be called on \textit{unknown} and \textit{unvisited} states/state-action pairs; however this would introduce a hyperparameter, $m$, to define after how many observations a state/state-action pair becomes known.
\subsection{Task-Agnostic Exploration}
The exploration that we perform is goal directed and task-specific. This could mean that in larger, more complex domains, generalisation is not possible. Therefore, an interesting line of work could modify the exploration approach slightly to be task-agnostic; that is the environment is explored without considering any specific task, as in \cite{plan2explore}. This could be done by the agent using intrinsic motivation to choose goals to plan and explore towards (utilising the Meta Actions along the way), as an example a goal could be chosen because its horizon is large (e.g. it seems like it will take many time steps to get there), and the planner may use the Meta Actions to hypothesise that actually it is much smaller. If this task-agnostic exploration is done correctly, it could mean that we can generalize to a variety of tasks in an environment with minimal learning beyond the initial exploration steps.

\subsection{Learning Meta Actions}
The method of learning Meta Actions that we proposed is rather simple, and may not actually be that effective - a better solution might be missed out on, as a discrepancy was not experienced, and thus Meta Action learned, that allowed the it to be discovered. For example, we may learn a Meta Action that prevents a transition to take place (due to some obstacle, for instance), but we might not learn the inverse that enables us to hypothesise that an obstacle is not in-fact there, and a transition is possible. As an alternative, we may not directly learn Meta Actions but employ a Neural Operator trained on experience in an environment, that takes as input a state, and suggests modifications to the transitions and rewards relating to that state.


% The ideas we have for future work mostly begin by solving the limitations outlined above. Most issues regarding scalability can be overcome by moving from exact to approximate solutions through function approximation methods; this would allow scaling to continuous, partially  observable domains. Furthermore, our method could be easily extended to domains with stochastic rewards, however in stateless domains such as Bandits, the criteria for calling a Meta Action that would increase reward needs to be further considered; this could be done by count-based methods or uncertainty based methods. Furthermore, more suitable planning algorithms could be explored, such as UCT \cite{10.1007/11871842_29}; which is the UCB algorithm \cite{auer2002finite} applied to tree search; this would offer a scalable, faster solution than Value Iteration. Consider generalization?
% \\Defining reasonability by known, unknown unvisited, similar to $E^3$, after the planning phase, don't visit any unvisited states?
