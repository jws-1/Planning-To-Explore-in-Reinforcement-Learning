\chapter{Methods}
\label{chapter3}
When human's make decisions, they are often driven by previous experiences and some form of internal model of the world that we maintain. However, sometimes we question ourselves and think "what if this other decision is better?"; this leads to us making different decisions, which are driven by optimistic reasoning as well as intrinsic motivation. As an example, consider the route that you travel to work everyday. At some point you might question yourself and think "what if there is a faster route?", which may lead to you taking a longer route, by distance, because you think that there is a possibility that it could be faster. It might turn out that the longer route is indeed slower, and you may never try it again. However, it might be that you realise that the longer route is much faster, due to lack of congestion for example, and thereafter you follow it. This is the decision-making and reasoning process that we try to emulate.
\\Therefore, we propose a framework that synthesises planning and reinforcement learning which aims to overcome the inherent inaccuracies of models whilst constraining exploration  and improving sample efficiency by hypothesising, through reasoning, changes to the model that would be most beneficial to the planner and then planning on this temporary model to guide exploration. The correctness of these changes is then realised through experience in the real environment, and the model is updated.
Exploration is driven by the principle of \textit{optimism in the face of uncertainty}, intrinsic motivation and extrinsic rewards. The agent is in a state of uncertainty due to the inevitable model inaccuracies. The planner is optimistic in its hypotheses, which are driven by the intrinsic motivation to question the accuracy of the model, while still planning to achieve maximum return. To enable the planner to make these hypotheses we equip it with additional actions which we denote Meta Actions.

% This approach means that exploration is driven by both the intrinsic motivation of the agent and extrinsic rewards; the agent explores with the goal of finding the optimal policy, whilst optimistically contemplating that there might be better policies. This is based on the principle of \textit{optimism in the face of uncertainty} \cite{lai-allocation}; until we have actually verified the correctness of the model through experience in the real environment, we are in a state of uncertainty due to the inevitable inaccuracies.

\section{Meta Actions}
\label{sec:31}
These actions do not cause the agent to act in and affect the environment, and thus do not return any observation in terms of a new state and a reward, but rather cause changes directly to the model when called upon. An important factor and one of the main difficulties is deciding what Meta Actions should be exposed to the planner and when the planner should be able to invoke the Meta Actions. Therefore, we state three conditions which all must hold for a Meta Action to be invoked: a Meta Action must be admissible, feasible and reasonable.

\begin{defn}
\label{defn:admissible}
    A Meta Action is admissible if applying it to the model leads to a better policy with respect to the model.
\end{defn}
It's important that hypothesised changes are optimistic; that they benefit the planner in some way, otherwise they are not very useful. Thus, admissibility is important, as defined in Definition \ref{defn:admissible}, if applying a Meta Action does not result in any benefit to the planner, but rather it negatively affects the planner, then it should not be called. This means that planning must be done with finding the sequence of actions that maximises reward in mind.
\begin{defn}
\label{defn:feasible_deterministic}
In a deterministic domain, a Meta Action is feasible if the target state-action pair that it is to be applied to has not been previously observed nor has that Meta Action been previously called on it.
\end{defn}
\begin{defn}
\label{defn:feasible_stochastic}
In a stochastic domain, a Meta Action is feasible if either:
\begin{itemize}
    \item The target state-action pair that it is to be applied to has not been previously observed within the current episode nor has that Meta Action been called on it within the current episode.
    \item The target state-action pair that it is to be applied to has not been previously observed within the current or previous $N$ episodes, nor has that Meta Action been called on it within the current or previous $N$ episodes.
\end{itemize}
\end{defn}
If the agent were to make hypothesises that contradict its own observations, it would induce hallucinatory behaviour. Furthermore, by the optimistic nature of our approach, its possible that the agent could infinitely hypothesise changes to the model, namely in the form of increased rewards. Therefore, feasibility is important, as defined in Definitions \ref{defn:feasible_deterministic} and \ref{defn:feasible_stochastic}. Within deterministic settings the transitions and rewards that the agent observes are the true ones; thus we can be certain that the model is correct with respect to the observations of the agent, hence Definition \ref{defn:feasible_deterministic} ensures that observations in the entire lifetime of the agent are not contradicted. Definition \ref{defn:feasible_stochastic} gives two alternate conditions for feasibility in stochastic settings, each of which are explored throughout the implementations to see which is stronger. This definition ensures that the agent cannot contradict recent observations (those within the current or previous $N$ inclusive episodes), and thus hallucinate, whilst ensuring that changes to the model cannot be infinitely hypothesised. It allows Meta Actions to be called multiple times on state action pairs, including those of which have been previously observed, this is because we can never be completely certain of the correctness of the model due to the aleatoric uncertainity introduced due to the stochasticity.

% Definition \ref{defn:feasible_deterministic} ensures that in deterministic settings the agent cannot contradict its observations, and thus hallucinate, whilst ensuring that changes to the model cannot be infinitely hypothesised.

\\
\begin{defn}
\label{defn:reasonable}
A Meta Action is reasonable if it has been embedded by-hand or learned through experience.
\end{defn}
Often, the change to the model that would be of most benefit the planner, is to simply add a transition from the current state to the goal state. However, this is almost never going to be a change that is realised to be correct, it is too optimistic, and instead would lead to behaviour akin to taking a random-walk through the state space until the goal is reached and the Meta Actions have been exhausted, or perhaps it would induce exploration that is akin to R-MAX or OIM. Hence, reasonability, as defined in Definition \ref{defn:reasonable} is key.

\section{The Framework}
We assume that the environment has a discrete state space, $S$, or a state space that can be discretised, a finite action space, $A$, with deterministic or stochastic dynamics which can be described by a transition function, $T$, and deterministic rewards which can be described by a reward function, $R$. Therefore, we assume that the environment can be modelled as an MDP $E = (S, A, T, R)$, within which the agent acts at discrete time steps. The goal of the framework is to produce a policy, $\pi^*$ which maximises the cumulative reward received when acting in the environment $E$.
Additionally, we assume that an attempt at modelling the environment has been made and embedded in an MDP $M = (S, A, T', R')$, where the same conditions hold for $T'$ and $R'$ as for $T$ and $R$. We do not expect or require $M$ to be accurate. 
\\We perform model-learning by maintaining $T'$ as a tabular maximum-likelihood model, and keep track of observed transitions using a function $n$ which maps state-action-state triples to an integer indicating how many times that transition has been observed.
\\We learn a tabular Q-Function, $Q$, through Q-Learning, which the policy $\pi$ is derived from. Q-Learning was chosen over SARSA, since it allows the optimal policy to be learnt independently of the current policy being followed; this suited our framework well, since we acknowledge that the policies followed during the exploration may not be optimal.
% The choice of initialisation open.
\\Our framework consists of two distinct phases: a planning phase where exploration takes place and a model-free phase. Since we only consider episodic tasks, the agent is given a finite number of episodes, $N_p$, for the planning phase, thereafter until termination (some finite number of episodes has been completed), the model-free learning takes over and the idea is that given sufficient model-free episodes $Q$ can converge to $Q^*$, and thus $\pi^*$ can be derived. Algorithm \ref{alg:framework_pc} gives a relatively high-level overview of the framework.

\begin{algorithm}
\caption{High-level Framework Pseudocode}
\label{alg:framework_pc}
\begin{algorithmic}
\REQUIRE $N$, number of episodes
\REQUIRE $N_p$, number of planning episodes
\REQUIRE $M=(S,A,T,R)$, model
\REQUIRE $s_s$, $s_g$, start, goal state
\ENSURE $\pi$, final policy
\STATE $i \leftarrow 0$
\STATE $Q(s,a) \leftarrow 0 $, $\forall s \in S$, $\forall a \in A$
\STATE $s \leftarrow s_s$
\FOR{$i=0$ to $N$}
    \STATE $Planning \leftarrow (i < N_p)$
    \WHILE{$s \neq s_g$}
        \IF{$Planning$ is \TRUE}
            \STATE $a \leftarrow$ Plan($s$, $s_g$)
        \ELSE
            \STATE $a \leftarrow \argmax_a Q(s,a)$
        \ENDIF
        \STATE Take action $a$ in state $s$, observe reward $r$ and new state $s'$
        \STATE Learn $Q$ according to observation.
        \IF{$Planning$ is \TRUE}
            \STATE Learn $M$ according to observation.
        \ENDIF
        $s \leftarrow s'$
    \ENDWHILE
\ENDFOR
\STATE $\pi(s) \leftarrow \argmax_a Q(s,a), \forall s \in S, \forall a \in A$
\RETURN $\pi$
\end{algorithmic}
\end{algorithm}


% \\Our framework consists of two phases, a planning phase and a model-free phase. 
%  Since we only consider episodic tasks, the agent is given a finite number of episodes, $N_p$ for the planning phase. The planning phase performs exploration and computes initial estimates for the Q-Values, $Q$. The model-free phase bootstraps from the initial Q-Value estimates and carries out model-free learning, following the policy $\pi_Q$, refining the estimate and continually updating the Q-Values.
\subsection{The Planning Phase}
The planning phase has three distinct steps: planning, acting and learning, which can be seen in Figure \ref{fig:planning_phase}. The goal of the planning phase is to perform exploration, and provide a good estimate for $Q^*$ which the model-free learning can bootstrap and derive a policy from.

\begin{figure}[h!]
    \centering
    \includegraphics[max size={\textwidth}{\textheight}]{report/assets/planning_phase.png}
    \caption{Planning Phase}
    \label{fig:planning_phase}
\end{figure}

% Throughout the planing phase we maintain a tabular likelihood model which is used for model-learning, therefore we maintain a table $n$ which maps state-action-state triples to integers; for instance $(0, 1, 0) \rightarrow 3$ means that the transitions $(0, 1, 0)$ has been observed three times.
\subsubsection{Planning}
The planner constructs a temporary model, $M'$, which is identical to $M$. It then plans on $M'$ to produce a plan, $P$, from the current state $s$ to some goal, terminal, state $s_g$. The planner has access to the action space, $A$, as well as the additional Meta Actions. Whilst considering the Meta Actions, the planner may create additional temporary models, in order to evaluate the benefit of calling a particular Meta Action. If a Meta Action is chosen, then the temporary model $M'$ is updated, and planning continues. $P$ is maintained by the planner until $M$ is updated, when re-planning occurs. This ensures that unnecessary planning does not take place, saving on computational costs. However, this means that some mechanism needs to be in place for determining if the model has been altered since the last plan was generated; this can be a simple Boolean flag. $P$ is stored in a first-in, first-out (FIFO) data structure, such as a Queue. Thus, when the planner is invoked by the agent it simply removes and returns the top action.
\subsubsection{Acting}
At discrete time steps, $t$, the agent samples an action $a$ from the Planner, and executes it. If the action is a Meta Action, then it did not result in any interactions with the environment, and thus no observations are "time steps" are made. Otherwise, at time $t+1$ it observes its new state $s'$ and the scalar reward signal
\subsubsection{Learning}
Learning only occurs of the action taken was not a Meta Action. The observation table is updated with the observed transition: $n(s, a, s') \leftarrow n(s, a, s')+1$, and if necessary the transition function, $T'$ of $M$ is updated by Equation \ref{eqn:tmlmupdate}. Furthermore, the reward function, $R'$, of $M$ is updated with the received reward, if necessary. $Q$ is updated according to the new state and reward received using Equation \ref{eqn:qlearningupdate}.
% We begin by updating the observation table $n$ with the observed transition. Then, we update the model, $M$ (if necessary). Furthermore, the $Q$-Function is updated according to the new state and reward received.
\subsection{The Model-Free Phase}
The model-free phase has two distinct steps: acting and learning, which can be seen in Figure \ref{fig:model_free_phase}. The goal of the model-free phase is to use pure model-free learning to bootstrap from the Q values learned during exploration, and get as close as possible to $Q^*$, so that $\pi^*$ can be derived.

\begin{figure}[h!]
    \centering
    \includegraphics[max size={300}{300}]{report/assets/model_free_phase.png}
    \caption{Model-Free Phase}
    \label{fig:model_free_phase}
\end{figure}

\subsubsection{Acting}
At discrete time steps, $t$, the agent greedily selections an action $a$ with respect to the $Q$, it selects the action according to the current policy, and executes it. At time $t+1$ it observes its new state $s'$ and the scalar reward signal.
\subsubsection{Learning}
$Q$ is updated according to the new state and reward received using Equation \ref{eqn:qlearningupdate}. Eventually, the updates will not result in changes to the policy, $\pi$, and therefore it will be continually followed until termination.
\section{An Illustrative Example}
Consider the deterministic domain in Figure \ref{fig:cliff-real}. This is a modified version of the cliff-walking domain \cite{Sutton1998}. The red circle located at (0,0) is the agent; the goal is located in the bottom right corner, (0, 7). If the agent transitions into one of the "cliff" states, they are returned to the start state. Let's suppose that for some reason, perhaps due to changes in the environment, the agent is seeded with the inaccurate model shown in Figure \ref{fig:cliff-model}. A pure planning approach would fail, as it would continually plan a path that goes through the cliff, due to the inaccurate model. A pure model-free learning approach would probably be successful, as this is a very simple domain, however in reality domains can be much more complicated than this; which is where model-free methods begin to struggle.

\begin{figure}[h!]
    \centering
    \includegraphics[max size={200}{200}]{report/assets/envs/cliff_real.png}
    \caption{Modified Cliff-Walking Domain}
    \label{fig:cliff-real}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[max size={200}{200}]{report/assets/envs/cliff_model.png}
    \caption{Agent's Model}
    \label{fig:cliff-model}
\end{figure}


\\Assuming an implementation of our framework, where reasonable Meta Actions are embedded, that allow the agent to hypothesise changes to transitions and rewards originating in the current state, and targeting an adjacent state. 
Initially, the most beneficial changes to the model would be to remove the cliff across the bottom row, as shown in Figure \ref{fig:cliff-hyp-1}. The agent then attempts to follow the plan going across the bottom of the grid, after which it realises that the hypotheses were correct. The planner may make further hypotheses which lead to the agent trying alternate paths, for instance hypothesising that the cliff is not present across the second row and that the reward through that row is increased; meaning that it would be a better path than the previous one. This process continues, with the planner making hypotheses and the agent verifying them, until no more hypotheses can be made, or the agent runs out of planning steps, after which the model-free learning takes over, and the produce policy matches the initial plan.

\begin{figure}[h!]
    \centering
    \includegraphics[max size={200}{200}]
    {report/assets/envs/cliff_hypothesis_1.png}
    \caption{Hypothesised Model}
    \label{fig:cliff-hyp-1}
\end{figure}

\section{Implementations}
The high-level ideas of the framework led to various implementations. The main differences between the implementations lie in the choice of planning algorithm, thus how the planning algorithm hypothesises changes, the available Meta Actions and the source of reasonable Meta Actions, for example learned versus embedded. Furthermore, some implementations had simplifications applied to them to deal with specific domains. We note that these implementations are not definitive, and much improvements could be made, but they are designed with the goal of proving the usefulness of Meta Actions. All implementations were developed in Python. Whilst C or C++ would have been a better choice for computational reasons, various RL benchmarking suites are available for Python.
% The description of the framework above led to various implementations. The underlying concept of allowing the planner to hypothesise changes to the model through Meta Actions is present throughout all of the implementations. However, the choice of planning algorithm differs. Each implementation will be presented and highlighted here. Since in general the implementations follow the high-level overview provided in Algorithm \ref{alg:framework_pc}, we will not provide pseudocodes for each individual implementation, but we will however note where implementations differ.
\subsection{RL-A* Meta}
This was the intial implementation of the framework, and we developed with the goal of realising if Meta Actions are useful, and if so, when they are useful. The chosen planner was a basic A* planner, which limited the implementation to deterministic domains. A* was chosen due to ease of implementation and its use of an evaluation function, $f$, which provided a good means of evaluating Meta Actions. As discussed in Section \ref{sec:astar}, the evaluation function, $f$, is the combination of the heuristic function, $h$, and the cost function. For the cost function, it was intuitive to utilise the reward function. Namely, we define the cost of being in a state, $s$, as the sum of inverted rewards that it took to arrive at that state. Namely:
\begin{equation}
\label{eqn:astarval}
f(s_t) = -\sum_{k=0}^{t-1}\Bigg[R(s_k, a_k, s_{k+1})\Bigg] + h(s_t)
\end{equation}
The choice of heuristic, $h$, relies on domain specific knowledge, therefore we do not define it here. However,  it remains that the heuristic must be admissible.
\\The Meta Actions that were available to the planner allowed it to add/remove transitions, and increase/decrease rewards for state-action-state triples, reasonable meta actions were embedded in the model by-hand. The transition function was treated as a graph, which A* search was then performed on. To ensure feasibility of Meta Actions, a table was maintained which kept track of which Meta Actions had been called on which state-action-state triples - this was used to ensure that each Meta Action can only be called once on each state-action-state triple. Admissibility was handled naturally by A*.
\subsection{RL-A* Meta with short-term memory}
\label{sec:342}
This implementation was an extension of RL-A* Meta, which aimed to scale to stochastic domains. The stochastic nature meant that the evaluation function once again needed to be modified, as such:
\begin{equation}
\label{eqn:astarevalsast}
f(s_t) = -\sum_{k=0}^{t-1}\Bigg[(1-T(s_k, a_k, s_{k+1}))R(s_k, a_k, s_{k+1})\Bigg] + h(s_t)
\end{equation}
Since transitions were not guaranteed, the cost was weighted using the probability of the transition not occurring. This meant that transitions with a higher probability of occurring were preferred.
\\ The Meta Actions that were available to the planner allowed it to increase/decrease transition probabilities and increase/decrease rewards for state-action-state triples. Reasonable meta actions were embedded in the model by-hand. 
To ensure feasibility of Meta Actions, a table was maintained which kept track of which actions had been called on which state-action-state triples within the previous $N$ episodes; we refer to this as the short-term memory. This encouraged the agent to try Meta Actions again that it had tried in the past, but "forgotten" that it had done so; if it got unlucky previously due to stochasticity, it could try again and discover a good policy it may not have been able to discover before.
% By the definition of feasibility in RL-A* Meta, if we hypothesise a change which would be beneficial, and then follow a path that utilises that change but realise that the change didn't come to fruition due to the agent getting unlucky with stochasticity, we would never try that path again. Therefore, we needed to alter this definition to still ensure that Meta Actions are not infinitely called but also to ensure that they can be tried multiple times, in case the agent got unlucky due to stochasticity. Hence, a table was maintained which kept track of which Meta Actions had been called on which state-action-triples in the previous $N$ episodes, this was referred to as the short term memory. 
\subsection{RL-VI Meta}
The main problem with the RL-A* agents is the reliance on a good heuristic function, which can be difficult to design and hence, they are limited to domains where a good heuristic function can be easily designed.
Therefore, this implementation aims to deal with varying domains. We opted for planning by dynamic programming, namely through Value Iteration. Value Iteration was chosen because it allowed for us to easily evaluate plans (policies) through the Value Function. We chose Value Iteration over Policy Iteration, as Value Iteration is generally faster, and we needed to perform it many times. Despite its nature, in our setting repeatedly performing Value Iteration is not too expensive, as the changes (both real and hypothetical) to the model are not too different, which means that Value Iteration can converge in few iterations.
\\The Meta Actions that were available to the planner allowed it to increase/decrease transition probabilities, and increase/decrease rewards for state-action-state tuples, reasonable meta actions were embedded in the model by-hand. 
% Starting from an initial estimate of the optimal value function, $V$, the planner constructs a plan that is greedy with respect to $V$, however at each state, $s$, changes are hypothesised and verified to be beneficial by performing a few steps of value iteration to produce a temporary value function $V_h$. If $V_h(s) > V(s)$, then the change is accepted and the Meta Action is added to the plan.
To ensure feasibility of Meta Actions and ensure that they were not infinitely called, a table was maintained which kept track of those called within the current episode.
% A brief overview of the planner implementation can be seen in Algorithm \ref{alg:vi_planner}.
\subsection{RL-VI Meta, with learned Meta Actions}
The overall implementation is the same as RL-VI Meta, except Meta Actions are learned and obtained through experience, rather than embedded by-hand in the model. A Meta Action is learned when a discrepancy is noticed between the model and the real environment; this change that was applied to the model through model-learning becomes an action that can be invoked later on. For example, if an action "UP" is taken, which the model expects to move the agent a single state in the upward direction, but actually mvoes the agent two states upwards, then the agent learns the Meta Action that adds a transition to move two states upwards on the "UP" action.


% \begin{algorithm}
% \caption{VI Planner}
% \label{alg:vi_planner}
% \begin{algorithmic}
% \REQUIRE $V$, initial value function
% \REQUIRE $M=(S,A,T,R)$, model
% \REQUIRE $s_s$, $s_g$, start, goal state
% \ENSURE $Plan$


% \STATE $M' \leftarrow M$
% \STATE $s \leftarrow s_s$
% \STATE $Plan \leftarrow []$
% \WHILE{$s \neq s_g$}
%     \STATE $C \leftarrow \FALSE$
%     \FOR{Each possible change}
%         \STATE $M_h \leftarrow $ copy of $M'$ with change applied.
%         \STATE $V_h \leftarrow ValueIteration(M_h)$
%         \IF{$V_h(s) > V(s)$}
%             \STATE $C \leftarrow \TRUE$
%             \STATE $M_b \leftarrow M_h$
%             \STATE $V_b \leftarrow V_h$
%             \STATE $A_M \leftarrow$ Meta Action that caused $M_h$
%         \ENDIF
%     \ENDFOR
%     \IF{$C$ is \TRUE}
%         \STATE $M' \leftarrow M_b$
%         \STATE $V \leftarrow V_b$
%         \STATE $Plan.add(A_M)$
%     \ENDIF
%     \STATE $A \leftarrow$ greedy action in state $s$ with respect to $V$
%     \STATE $Plan.add(A)$
%     \STATE $s \leftarrow$ new state after taking action $A$
% \ENDWHILE
% \RETURN $Plan$




% \STATE $i \leftarrow 0$
% \STATE $Q(s,a) \leftarrow 0 $, $\forall s \in S$, $\forall a \in A$
% \STATE $s \leftarrow s_s$
% \FOR{$i=0$ to $N$}
%     \STATE $Planning \leftarrow (i < N_p)$
%     \WHILE{$s \neq s_g$}
%         \IF{$Planning$ is \TRUE}
%             \STATE $a \leftarrow$ Plan($s$, $s_g$)
%         \ELSE
%             \STATE $a \leftarrow \argmax_a Q(s,a)$
%         \ENDIF
%         \STATE Take action $a$ in state $s$, observe reward $r$ and new state $s'$
%         \STATE Learn $Q$ according to observation.
%         \IF{$Planning$ is \TRUE}
%             \STATE Learn $M$ according to observation.
%         \ENDIF
%         $s \leftarrow s'$
%     \ENDWHILE
% \ENDFOR
% \STATE $\pi(s) \leftarrow \argmax_a Q(s,a), \forall s \in S, \forall a \in A$
% \RETURN $\pi$
% \end{algorithmic}
% \end{algorithm}