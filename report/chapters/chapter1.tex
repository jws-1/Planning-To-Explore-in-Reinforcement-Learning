
\chapter{Introduction and Background Research}

\label{chapter1}
\section{Introduction}

Reinforcement Learning (RL) \cite{DBLP:books/lib/SuttonB98} is based on the idea of learning through experience, it is a form of trial-and-error learning. A decision-making agent learns how to behave in an environment by interacting with it and receiving positive and negative reinforcement. This is akin to human and animal learning, and comes from the field of Psychology through the studies of operant conditioning \cite{nla.cat-vn2770732}, which have shown that human and animal behaviour can be shaped through positive and negative reinforcement.
\\Experience can only be gained by trying new things, however it is often infeasible to try everything. Hence, there is a need for exploration, where the agent tries new things and learns the consequences of its actions. However, indefinitely exploring means that the experience gained is never exploited - this gives rise to the so-called exploration versus exploitation dilemma. 
\\Exploration is a very important topic in RL, as it necessitates learning. However, to effectively learn, exploration must be efficient in terms of learning time and cost \cite{Thrun-1992-15850}. Although exploration methods exist based on optimism, such as UCRL \cite{NIPS2006_c1b70d96} and Fitted R-Max \cite{SARA07-jong}, intrinsic motivation, such as ICM \cite{DBLP:journals/corr/PathakAED17}, confidence bounds, such as \cite{10.5555/911176}, and Bayesian methods, such as Bayesian Q-Learning \cite{10.5555/944919.944941}, in practice random exploration, such as $\epsilon$-greedy \cite{Watkins:1989, conf/nips/Sutton95}, is ubiquitous. Random exploration precludes efficiency, since non-optimal behaviour is continually evaluated, long after it has been realised to be non-optimal. Furthermore, randomness implies a lack of intelligence; an intelligent agent should not exhibit random behaviour.
\\Automated Planning (or just Planning) \cite{russelNorvig2003:aima, Lav06} uses embedded knowledge of the environment, in the form of a model, to determine the optimal sequence of successive actions to fulfil a goal . However, planning relies on the model to accurately represent the environment; which cannot be guaranteed, due to approximations and abstractions, and therefore planning can be quite fragile. A clear distinction between Planning and RL is that Planning relies on previously obtained knowledge, whereas RL relies on obtaining knowledge through experience.
\\Although Planning and RL take different approaches to decision making, they may be combined, which is known as Model-Based RL (MBRL). Most commonly MBRL is encapsulated in the form of planning over a learnt model, which has been shown to be very successful in recent years  \cite{silver2017mastering, pmlr-v28-levine13}. MBRL may also be used to guide exploration, such as DARLING \cite{AIJ16-leonetti}, UBE \cite{DBLP:journals/corr/abs-1709-05380} and MAX \cite{DBLP:journals/corr/abs-1810-12162}, this is the main investigation of this work.
\\Within this work we explore the development of a framework that synergises planning and learning in order to drive and constrain exploration by making intelligent hypotheses about the environment, informed by the inherently inaccurate, but still useful, model, previous experience and environmental observations, rather than through randomness. The ultimate goal of this work is to mitigate the effect of the inherent inaccuracies in the model on the quality of learned behaviour; resulting in agents that can learn beyond the inaccuracies of the model, through intelligent exploration.

\section{Reinforcement Learning}
Reinforcement Learning (RL) does not fall into either of the traditional machine learning paradigms (supervised and unsupervised learning) - it is a machine learning paradigm of its own. A RL problem comes in the form of a sequential-decision making problem, and is formalised through a Markov Decision Process (MDP). Within an RL problem a goal-directed decision-making agent learns how to behave in an environment, which may be stochastic. The agent learns by interacting with the environment through actions and observing the affects through its new state and a numerical reward signal. The goal of the agent is to learn how to map states to actions in order to maximise the cumulative long-term reward signal \cite{DBLP:books/lib/SuttonB98}. The behaviour that the agent learns is known as the \textbf{policy}. The \textbf{reward function} indicates the immediate value of state-action pair, the goal of the agent is to maximise the cumulative returns from this. The \textbf{value function} indicates the expected cumulative reward the agent can receive starting from a given state.
\\Within this work, we split RL into two categories: model-free and model-based. Model-free RL is the traditional instantiation of RL - the agent learns to act in an environment, with no knowledge of its dynamics. We briefly mentioned model-based RL within the introduction, but didn't explain what it actually is. Model-based RL is where the agent learns to act in an environment, and has some understanding of the dynamics of the environment in the form of a model. By the nature of models, the model is inaccurate, more often than not.

\subsection{Markov Decision Processes}
The Markov Property states that the future is conditionally independent of the past given the present. An RL problem that satisfies the Markov Property is known as a Markov Decision Problem, and can be modelled by a Markov Decision Process (MDP). Where an agent is able to fully observe its state, the problem can be modelled as an MDP. Conversely, where an agent can only partially observe its state, the problem can be modelled by a Partially Observable Markov Decision Process (POMDP). Consider an agent interacting with a stochastic gridworld, the agent can easily observe its state, whereas a robot navigating a maze may not be able to observe its exact state, due to uncertainty in sensors, joint readings, etc. In fact, the real-world is a POMDP. Within this work, as a simplification, we assume that the agent is fully able to observe its state. Furthermore that the environment can be discretised (rather than being modelled in a continuous nature); hence we consider \textbf{finite} MDPs.
A finite MDP is a 5-tuple: $\text{MDP} = (S,A,T,R, \gamma)$ where:
\begin{itemize}
    \item $S$ is a finite set of states.
    \item $A$ is a finite set of actions.
    \item $T : S \times A \times S \rightarrow [0,1]$ is the transition function, which determines the probability of transitioning from a state $s \in S$ to $s' \in S$ with an action $a \in A$.
    \item $R:S \times A \times S \rightarrow \mathbb{R}$ is the reward function, which determines the reward signal, $r \in \mathbb{R}$ received by the agent from transitioning from a state $s \in S$ $s' \in S$ with the action $a \in A$. This reward is extrinsic to the agent; it comes from the environment.
    \item $0 \le \gamma \le 1$ is the discount factor, which indicates the importance of future rewards compared to immediate rewards.
\end{itemize}
The transition function, $T$ is a key indicator about the nature of the environment.
If $\forall s,s' \in S, \forall a \in A$, $T(s,a,'s) \in \{0,1\}$, then the environment is deterministic, otherwise it is stochastic.
A deterministic environment is one which there is no variance in the outcomes of action in a given state; taking the same action in the same state always produces the same outcome. Whereas a stochastic (or non-deterministic) environment has uncertainty associated with transitions.
\\The Bellman Equation determines the expected reward for being in a state $s \in S$ and following a fixed policy $\pi$:
$$V^\pi(s) = R(s, \pi(s)) + \gamma \sum_{s'}P(s'|s,\pi(s))V^\pi(s')$$
Where $V^\pi$ is the value function of the policy, $\pi$, $0 \le \gamma \le 1$ is the discount factor.
The Bellman Optimality equation determines the reward for taking the action giving the highest expected return.
$$V^{\pi*}(s) = \text{argmax}_a{R(s,a) + \gamma\sum_{s'}P(s'|s,a)V^{\pi*}(s')}$$
Where $V^{\pi*}$ is the value function of the optimal policy, $\pi*$, $0 \le \gamma \le 1$ is the discount factor.
% \subsection{Dynamic Programming}
% \begin{itemize}
% \item Given a perfect model of the environment, embedded in a MDP, an optimal policy can be computed using Dynamic Programming. However, this assumption of a perfect model is flawed.
% \end{itemize}
% \subsubsection{Policy Iteration}
% \subsubsection{Value Iteration}
\subsection{Temporal Difference Learning}
The (temporal) credit assignment problem \cite{Minsky:1961:ire} is the problem of determining which actions led to an outcome, and assigning credit among them; it's often the case that a sequence of actions led to an outcome, rather than a single action. In the context of RL, temporal credit assignment is important because in order to maximise the cumulative long-term reward, the agent needs to know which actions will realise such outcome. Temporal Difference (TD) \cite{10.5555/911176, 5392560} learning uses this concept; learning is driven by the error/difference between temporally successive predictions, so learning occurs whenever there is a change in prediction over time. It's a method for learning to predict; learning a prediction from another later learned prediction.
TD Learning algorithms can be on-policy, where the value of the policy being currently carried out by the agent is learnt, or off-policy, where the value of the optimal policy is learnt independently of the agent's actions \cite{PooleMackworth17}.
\subsubsection{Q-Learning}
Q-Learning \cite{Watkins:1989, journals/ml/WatkinsD92} is an off-policy Temporal Difference Learning method. It learns an estimate of the expected cumulative reward for each state-action pair, this is known a the Q-value. Q-values are iteratively updated using the Bellman equation, the update rule is as follows:
$$Q(s_t,A_t) \leftarrow Q(s_t,a_t) + \alpha[r_{t+1} + \gamma \max_aQ(s_{t+1}, a) -Q(s_t,a_t)]$$
Where $0 \le \alpha \le 1$ is the learning rate, which indicates how quickly learning occurs.
\\Clearly Q-Learning comes from Dynamic Programming, and in that sense it is a tabular method; a Q-table is maintained which stores the Q-values for each state-action pair. For this reason, it doesn't scale too well to large state/action spaces - however it is suitable for the domains that we consider within this work.
\section{Planning and Searching}
\begin{itemize}
    \item Planning involves reasoning on a model of an environment in order to produce a sequence of actions that will achieve a goal.\cite{russelNorvig2003:aima}.
\end{itemize}
\subsection{A* Search}
\begin{itemize}
    \item A* Search \cite{4082128} is not necessarily a Planning algorithm, rather it is a search algorithm. However, a simple Planning agent with the purpose of navigation in a discretised state space can use A* for planning.
    \item Heuristic
    \item Admissable
\end{itemize}
\subsection{Dynamic Programming}
Dynamic Programming \cite{Bellman:1957, DBLP:books/lib/Bertsekas05} is a problem-solving method which involves breaking down problems into smaller sub-problems, solving them, storing the solutions, and combining them to solve the original problem. Given a perfect model of the environment, embedded in an MDP, an optimal policy can be computed using Dynamic Programming, through both Policy Iteration \cite{Bellman:1957, howard:dp} and Value Iteration \cite{Bellman:1957}. Policy Iteration... Value iteration...

\section{Exploration}
% Randomnes underlies most exploration strategies in RL, from using pure-randomness, to randomly samplin
\subsection{Random}
\subsubsection{$\epsilon$-greedy}
$\epsilon$-greedy \cite{Watkins:1989, conf/nips/Sutton95} aims to balance exploration and exploitation through an $\epsilon$ factor, such that the agent exploits its learned knowledge, taking the current best action, with probability $1-\epsilon$ and explores randomly with probability $\epsilon$. It's common to decay $\epsilon$ temporally, so that the agent explores a lot early on and then exploits more after it has learnt for a while. Whilst this method offers a nice balance between the two extremes of pure exploration and pure exploitation, it only provably converges to an optimal policy with an infinite number of observations, so in-fact it is quite inefficient. Furthermore, due to the random nature, it results in continually evaluating sub-optimal actions. A key drawback to $\epsilon$-greedy is that without infinite observations, it can easily get stuck in a local optima.
\subsubsection{$\epsilon z$-greedy}
Dabney, et al., \cite{dabney2021temporallyextended} stated that another key problem with $\epsilon$-greedy is its lack of temporal persistence; actions are only performed for one time-step. They proposed $\epsilon z$-greedy, or temporally extended $\epsilon$-greedy, where the agent takes the current best action with probability $1-\epsilon$ and follows an "option" until termination with probability $\epsilon$. An "option" is a sequence of actions.
\subsubsection{Boltzmann}
\subsubsection{Thompson Sampling}
\subsubsection{UCB}
\subsubsection{Conclusion}
\subsection{Intrinsically Motivated}
\subsubsection{Count-based}
\subsubsection{Information Theoretic}
\subsubsection{Curiosity Driven}
\subsection{Model-Based Exploration}
\subsubsection{DARLING}