\chapter{Background Research \& Notation}
\label{chapter2}
\section{Markov Decision Processes}
The Markov Property states that the future is conditionally independent of the past given the present. A sequential decision-making problem that satisfies the Markov Property is known as a Markov Decision Problem, and can be modelled by a Markov Decision Process (MDP) \citep{10.5555/528623}. Where an agent is able to fully observe its state, the problem can be modelled as an MDP. Conversely, where an agent can only partially observe its state, the problem can be modelled by a Partially Observable Markov Decision Process (POMDP).
Consider an agent playing a perfect information game \citep{vonneumann.morgenstern47}, such as Chess or Go, the agent (and its opponent) are able to fully observe the state of the game with ease. However, a robot navigating through a maze may not be able to observe its exact state due to uncertainty in its sensors.
\\Within this work we assume that the agent is fully able to observe its state. Furthermore that the environment can be discretised, through approximations, and tasks are episodic (there exist some terminal states which indicate the end of a task); hence we consider \textbf{finite}, \textbf{undiscounted} MDPs.
Thus, an MDP is a 4-tuple: $\text{MDP} = (S,A,T,R)$ where:
\begin{itemize}
    \item $S$ is a finite set of states.
    \item $A$ is a finite set of actions.
    \item $T : S \times A \times S \rightarrow [0,1]$ is the transition function, which determines the probability of transitioning from a state $s \in S$ to $s' \in S$ with an action $a \in A$.
    \item $R:S \times A \times S \rightarrow \mathbb{R}$ is the reward function, which determines the reward signal, $r \in \mathbb{R}$ received by the agent from transitioning from a state $s \in S$ to $s' \in S$ with the action $a \in A$. This reward is extrinsic to the agent; it comes from the environment.
\end{itemize}
The transition function, $T$ is a key indicator about the nature of the environment.
If $\forall s,s' \in S, \forall a \in A$, $T(s,a,'s) \in \{0,1\}$, then the environment is deterministic, otherwise it is stochastic.
A deterministic environment is one which there is no variance in the outcomes of action in a given state; taking the same action in the same state always produces the same outcome. Whereas a stochastic (or non-deterministic) environment has uncertainty associated with transitions.
\subsection{Policies, Value and Quality}
A policy is a mapping of states to actions that describes a behaviour within an MDP. A policy may have a Value Function associated with it, denoted $V_\pi$. For each state $s$, $V_\pi(s)$ indicates how good it is for the agent to be in that state; it is a measure of the expected reward the agent can receive starting in that state and following the policy $\pi$. A policy may also have a Quality Function associated with it, denoted $Q_\pi$. For each state-action pair $(s,a)$, $Q_\pi(s,a)$ indicates how good it is for the agent to take action $a$ in state $s$; it is a measure of the expected reward the agent can receive by taking action $a$ and then following the policy $\pi$.
% \subsection{Policies}
% A policy is a mapping of states to actions.
% \subsection{Value Functions}
% A Value Function, denoted $V(s)$ indicates how good it is for the agent to be in a given state, $s$. It is a measure of the expected reward the agent can receive starting in a state $s$, following a fixed policy $\pi$.
% \subsection{Quality Functions}
\subsection{Bellman Equations}
The Bellman Equation determines the expected reward for being in a state $s \in S$ and following a fixed policy $\pi$:
$$V^\pi(s) = R(s, \pi(s)) + \sum_{s'}T(s'|s,\pi(s))V^\pi(s')$$
Where $V^\pi$ is the value function of the policy, $\pi$.\\
The Bellman Optimality equation determines the reward for taking the action giving the highest expected return.
$$V^{\pi*}(s) = \text{argmax}_a{R(s,a) + \sum_{s'}P(s'|s,a)V^{\pi*}(s')}$$
Where $V^{\pi*}$ is the value function of the optimal policy.

\section{Planning and Searching}
Planning involves reasoning on a model of an environment in order to produce a sequence of actions that will achieve a specified goal \citep{DBLP:books/aw/RN2020, Lav06, GhallabNauTraverso04}.
\subsection{Heuristic Search}
Heuristic search uses heuristic functions to guide the exploration of possible solutions.
\subsubsection{A* Search}
A* search is a heuristic search algorithm, which is limited to discrete state and action spaces. It can be difficult to choose the heuristic, the heuristic must be admissable \citep{DBLP:books/aw/RN2020}; this means that it must always underestimate the cost of getting from the current state to the goal, and never overestimate it. An interesting extension of this is LRTA* \citep{KORF1990189}, which is akin to model-based RL.
\subsection{Dynamic Programming}
Dynamic Programming \citep{Bellman:1957, DBLP:books/lib/Bertsekas05} can be used to compute an optimal policy given a perfect model of the environment, embedded in an MDP \citep{Sutton1998}. If the model is not perfect, then the computed policy is only optimal for the model, not for the real environment; however this can still be useful. The two main algorithms that we consider are Policy Iteration \citep{Bellman:1957, howard:dp} and Value Iteration \citep{Bellman:1957}; both of which rely on the idea of Policy Improvement \citep{Bellman:1957}.
% Dynamic Programming \citep{Bellman:1957, DBLP:books/lib/Bertsekas05} is a problem-solving method which involves breaking down problems into smaller sub-problems, solving them, storing the solutions, and combining them to solve the original problem. Given a perfect model of the environment, embedded in an MDP, an optimal policy can be computed using Dynamic Programming, through both Policy Iteration  and Value Iteration.
\subsubsection{Policy Improvement}
Policy Improvement \citep{Bellman:1957} is the process of generating an improved policy from a suboptimal policy \citep{DBLP:books/lib/Bertsekas05}.
\subsubsection{Policy Iteration}
\citep{Bellman:1957, howard:dp}
\subsubsection{Value Iteration}
Value Iteration is a method for computing the optimal policy in an MDP; this policy is akin to a plan and can be used in the same way. Assuming an arbitrary Value Function, $V$, at each step $k$, $V$ is updated as such:
$$V(s)_{k+1} = \max_a\sum_{s'}P_{ss'}^{a}[R_{ss'}^a+V_k(s')]$$
\citep{series/synthesis/2010Szepesvari}
\section{Reinforcement Learning}
Within an RL setting, formalised by an MDP, an agent learns how to behave in an environment by interacting with it through actions, at discrete, sequential, time steps, and observing the affects through its new state and a scalar reward signal, as seen in Figure \ref{fig:rl}. The reward signal may be delayed, meaning that the consequences of actions may not be known until long after they are taken \citep{barto1990learning}. This gives rise to the (temporal) credit assignment problem \citep{Minsky:1961:ire}, the problem of determining which actions led to an outcome and assigning credit among them; it's often the case that a sequence of actions led to an outcome, rather than a single action.
The goal of the agent is to learn a policy that maximises the expected cumulative long-term reward \citep{Sutton1998}.
\begin{figure}[h!]
    \centering
    \includegraphics[max size={\textwidth}{\textheight}]{report/assets/rl.png}
    \caption{The RL Loop}
    \label{fig:rl}
\end{figure}
\subsection{Generalised Policy Iteration}
\subsection{Model-Free Learning}
Model-free (or direct) RL is the traditional instantiation of RL, where the agent learns a policy directly from experience gained by interacting with the environment; it is purely trial-and-error. Model-free learning tends to be quite flexible to varying problems, since no assumptions are made about the environment's dynamics. Furthermore, since the environment's dynamics do not need to be stored, learned or considered, model-free learning is scalable and does not suffer from the \textit{curse of dimensionality} (in the non-tabular cases) and can be computationally efficient, as there is generally no lookahead deliberation process involved. However, model-free learning can be sample-inefficient; the agent may need to take many actions before discovering the optimal policy.
\subsection{Model-Based Learning}
Model-based (or indirect) RL is where an agent uses a known or learned model of the environments dynamics (in the form of an MDP) in order to learn the optimal policy \citep{MAL-086, RLSOTA11}. The agent may learn the model and policy at the same time, as in the Dyna family \citep{Sutton:1990, 10.1145/122344.122377} or learn a policy by planning over a known model, as in AlphaZero \citep{DBLP:journals/corr/abs-1712-01815} or learn a policy by planning over a learned model, as in MuZero \citep{DBLP:journals/corr/abs-1911-08265}.
Model-based learning tends to provide better sample efficiency than model-free approaches, since learning can occur from simulation rather than directly from experience \citep{RLSOTA11} and the number of exploratory actions can be reduced by using targeted exploration \citep{Thrun-1992-15850}. However, this comes at a computational cost; planning can be expensive, especially if the state and action spaces are large.
% \\Comparison
% \begin{itemize}
%     \item model-free RL is more scalable, simpler to program
%     \item model-based RL can be more efficient, can overcome delayed rewards, however comes at a cost: planning takes time and computational power.
% \end{itemize}
\subsubsection{Model Learning}
Model learning can be viewed as a supervised learning problem \citep{JORDAN1992307}. In discrete environments, exact models of the environment's dynamics can be learned in the form of a \textit{tabular maximum likelihood model} \citep{10.1145/122344.122377} which essentially maintains a table storing every possible transition. In the stochastic case, for each transition the table will store:
$$T(s'|s,a) = \frac{n(s, a, s')}{\sum_{s'}n(s,a,s')}$$
Where $n(s,a,s')$ represents the number of times the transition has been observed.
The same can be used for the deterministic case, which will result in all transitions mapping to either 0 or 1. It should be noted that an environment could be discretised in order to learn an exact model, but in-fact this model will be approximate due to this discretisation.
% However, as with all tabular methods, this doesn't scale well.
A key drawback of this approach is the lack of scalability.
In continuous environments, tabular approaches are not possible due to the infinite number of states. The environment's dynamics can be approximately learned using Function Approximation.
% The agent learns to act in the environment by either learning or being provided with a representation of the dynamics of the environment.
% Model-based RL is where the agent learns to act in an environment, and has some understanding of the dynamics of the environment in the form of a model. By the nature of models, the model is inaccurate, more often than not. \citep{Sutton:1990, MAL-086, 10.1145/122344.122377, Kuvayev1996ModelBasedRL, RLSOTA11}
\subsection{Temporal Difference Learning}
Temporal Difference (TD) Learning \citep{10.5555/911176, 5392560, 5391906} aims at solving the problem of temporal credit assignment by combining ideas from Monte Carlo methods, which learn directly from experience in the environment, and Dynamic Programming methods, which bootstrap estimates from other previously learned estimates. It is a form of Generalised Policy Iteration, in that it aims to produce an optimal estimate of the Value Function, $V_\pi^*$, starting from an initial estimate, $V_\pi$, for a given policy, $\pi$,\citep{Sutton1998}.
\\The core idea of TD Learning is to update the estimate of the Value Function, $V_\pi$, whenever there is a difference between temporally successive predictions \cite{Sutton:1988}. This difference is known as the TD error. The updated estimate is bootstrapped from the previous estimate, meaning that TD learning essentially learns a prediction from another prediction.
\\The simplest TD Learning algorithm is TD(0) \citep{Sutton:1988}, which updates it's estimates as such:
$$V(s_t) \leftarrow V(s_t) + \alpha[r_{t+1} + V(s_{t+1}) - V(s_t)]$$
$r_{t+1} + V(s_{t+1})$ is the expected cumulative reward, known as the TD target, and $r_{t+1} + V(s_{t+1}) - V(s_t)$ is the TD error.
$0 \le \alpha \le 1$ is the learning rate, which is used to control how much weight is given to the TD error when updating the estimate. 
\\TD Learning can also be extended to learn a Q-Function, which stores the value of state-action pairs rather than the value of states. For each state-action pair, it stores an estimate of the expected cumulative reward starting in that state and taking an action, and following a fixed policy.
% \\Starting from an initial estimate of the Value Function, $V_\pi$, for the current policy being followed, $\pi$, the goal of TD Learning is 
% \\The core idea of TD learning is to learn the optimal estimate of the Value Function of the current policy being followed. The estimate is updated whenever there is a difference between the expected reward and actual reward received at each discrete time step; learning occurs whenever there is a difference between temporally successive predictions.
% The core idea of TD learning is to maintain an estimate of the Value Function of the current policy being followed, this estimate is updated whenever there is a difference between temporally successive predictions; this difference is known as the TD error.
% TD Learning algorithms can be on-policy, such as SARSA \citep{rummery:cuedtr94}, where the value of the policy being currently carried out by the agent is learnt, or off-policy, such as Q-Learning \citep{Watkins:1989, journals/ml/WatkinsD92}, where the value of the optimal policy is learnt independently of the agent's actions following the current policy.
% \citep{PooleMackworth17}.


% Within Temporal Difference (TD) \citep{10.5555/911176, 5392560, 5391906}, learning is driven by the error/difference between temporally successive predictions, so learning occurs whenever there is a change in prediction over time. It's a method for learning to predict; learning a prediction from another later learned prediction.
% The problem of temporal credit assignment gives rise to Temporal Difference (TD) Learning \citep{10.5555/911176, 5392560, 5391906}. TD Learning maintains an estimate of the Value Function, which is updated whenever there is a change in prediction over time.
\subsubsection{Q-Learning}
Q-Learning \citep{Watkins:1989, journals/ml/WatkinsD92} is an off-policy TD Learning method that learns a Q-function. It is off-policy, as the update rule assumes a greedy policy - this means that the value of the optimal policy is learned independently of the agent's actions following the current policy. Q-values are iteratively updated using the Bellman equation, the update rule is as follows:
$$Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha[r_{t+1} + \max_aQ(s_{t+1}, a) -Q(s_t,a_t)]$$
Where $0 \le \alpha \le 1$ is the learning rate, which indicates how quickly learning occurs.
% \\Clearly Q-Learning comes from Dynamic Programming, and in that sense it is a tabular method; a Q-table is maintained which stores the Q-values for each state-action pair. For this reason, it doesn't scale too well to large state/action spaces - however it is suitable for the domains that we consider within this work.
% \\The result of Q-Learning is that a deterministic, greedy policy is learned.
\subsubsection{SARSA}
SARSA (State Action Reward State Action) \citep{rummery:cuedtr94} is an on-policy TD Learning method that learns a Q-function. It is on-policy, as the update rule assumes that the current policy continues being followed - this means that the value of the current policy being followed is learned. Q-values are iteratively updated using the Bellman equation, the update rule is as follows:
$$Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha[r_{t+1} + Q(s_{t+1}, a_{t+1})-Q(s_t, a_t)]$$
Where $0 \le \alpha \le 1$ is the learning rate, which indicates how quickly learning occurs.
\section{Exploration in Reinforcement Learning}
Thrun \citep{Thrun-1992-15850} distinguished exploration methods in to two main categories: directed and undirected. Directed exploration refers to exploration that is informed by memory about the state space, whereas undirected exploration is uninformed, random. Due to the advances in exploration methods, these categories became redundant, and Amin et al's \citep{DBLP:journals/corr/abs-2109-00157} comprehensive survey on exploration in RL distinguished between reward-free and reward-based exploration; each of these categories is then broken down into memory-free (undirected) and memory-based (directed) exploration. We consider this taxonomy of exploration methods.
\subsection{Random Exploration}
Due to simplicity of implementation and ease of generalisation, random exploration is the most popular form of exploration, and is widely seen in the literature; even where some other exploration method is used, there is often some underlying randomness.
A Random Walk, or unguided random search \citep{anderson86}, arises from randomly sampling actions with uniform probability. This is perhaps the most naive method of exploration, since it is entirely random. The agent only explores and doesn't exploit; unless it learns a value function whilst exploring which it switches to exploit after some number of discrete time steps.
\\$\epsilon$-greedy \citep{Watkins:1989, conf/nips/Sutton95} uses a hyperparameter, $0 \le \epsilon \le 1$ to balance between exploration and exploitation. With probability $\epsilon$ the agent explores by taking a random action, with probability $1-\epsilon$ the agent exploits by taking the current best action. A key drawback of $\epsilon$-greedy exploration is that is lacks temporal persistence. Furthermore, by the nature of the name, it is a greedy method which could lead to a local optima.
\\$\epsilon z$-greedy \citep{dabney2021temporallyextended} is an extension to $\epsilon$-greedy that uses temporal persistence; the agent follows a sequence of actions (an option \citep{SUTTON1999181}) until termination with probability $\epsilon$ and with probability $1-\epsilon$ the agent exploits by taking the current best action.
% \begin{itemize}
%     \item Pure random exploration: Random Walk
%     \item $\epsilon$-greedy \citep{Watkins:1989, conf/nips/Sutton95}
%     \item $\epsilon z$-geedy \citep{dabney2021temporallyextended, SUTTON1999181}
%     \item Softmax
%     \item Boltzmann \citep{Watkins:1989, 10.1007/BF00992699, SCC.Barto.Bradtke.ea1991}
% \end{itemize}
\subsection{Intrinsically Motivated Exploration}
\citep{scott1996, pmlr-v97-jinnai19b, Jinnai2020Exploration}
\subsection{Optimistic}
\citep{10.1145/1390156.1390288, NIPS2006_c1b70d96, 10.1162/153244303765208377, NIPS2008_d5cfead9}
\subsection{Model-Based}
We refer to Model-Based exploration as any method of exploration that uses a model to influence exploration. For instance, a simple form of model-based RL where an agent plans on an initial model, and performs model-learning concurrently is an exploration method; although a naive one, that doesn't work in practice.\\
\citep{SARA07-jong, Littman2011EfficientME, Epshteyn2008ActiveRL}
\subsubsection{DARLING}
DARLING \citep{AIJ16-leonetti} computes a partial policy from an approximated deterministic model which it then performs $\epsilon$-greedy on. This leads to constraining exploration to a set of seemingly reasonable states, enabling the agent to overcome inaccuracies in the model, although the model is not explicitly learned. The main problem in this approach, is that it is only suitable for where the inaccuracies are not too significant, so there is a reliance on the embedded model being somewhat correct. Furthermore, randomness is present still.
\subsubsection{PEORL}
PEORL \citep{DBLP:journals/corr/abs-1804-07779}
\subsubsection{Plan-Q Learning}
\citep{10.1007/978-3-540-77949-0_6}
\subsubsection{FRAP}
\citep{DBLP:journals/corr/abs-2006-15009}
\subsubsection{Guided Dyna-Q}
\citep{Hayamizu2021GuidingRE}
\subsubsection{TMP}
\citep{Jiang2019TaskMotionPW}
\subsubsection{Simple Model-Based Exploration}
\begin{figure}[h!]
    \centering
    \includegraphics[max size={\textwidth}{\textheight}]{report/assets/diagram.png}
    \caption{Framework}
    \label{fig:framework}
\end{figure}
% Randomnes underlies most exploration strategies in RL, from using pure-randomness, to randomly samplin
% \subsection{Random}
% \subsubsection{$\epsilon$-greedy}
% $\epsilon$-greedy \citep{Watkins:1989, conf/nips/Sutton95} aims to balance exploration and exploitation through an $\epsilon$ factor, such that the agent exploits its learned knowledge, taking the current best action, with probability $1-\epsilon$ and explores randomly with probability $\epsilon$. It's common to decay $\epsilon$ temporally, so that the agent explores a lot early on and then exploits more after it has learnt for a while. Whilst this method offers a nice balance between the two extremes of pure exploration and pure exploitation, it only provably converges to an optimal policy with an infinite number of observations, so in-fact it is quite inefficient. Furthermore, due to the random nature, it results in continually evaluating sub-optimal actions. A key drawback to $\epsilon$-greedy is that without infinite observations, it can easily get stuck in a local optima.
% \subsubsection{$\epsilon z$-greedy}
% Dabney, et al., \citep{dabney2021temporallyextended} stated that another key problem with $\epsilon$-greedy is its lack of temporal persistence; actions are only performed for one time-step. They proposed $\epsilon z$-greedy, or temporally extended $\epsilon$-greedy, where the agent takes the current best action with probability $1-\epsilon$ and follows an "option" until termination with probability $\epsilon$. An "option" is a sequence of actions, akin to a "plan" in the planning literature.
% \subsubsection{Boltzmann}
% \subsubsection{Thompson Sampling}
% \subsubsection{UCB}
% \subsubsection{Conclusion}
% \subsection{Intrinsically Motivated}
% \subsubsection{Count-based}
% \subsubsection{Information Theoretic}
% \subsubsection{Curiosity Driven}
% \subsection{Model-Based Exploration}
% \subsubsection{DARLING}
% Leonetti et al \citep{AIJ16-leonetti}, developed DARLING (Domain Approximation for Reinforcement Learning), where given a model, a planner is used to produce a "partial policy", which is a set of reasonable choices the agent can make in each state. Exploration is then constrained to this partial policy, and performed using $\epsilon$-greedy. This work showed that whilst planning and RL take two different approches to decision making, on on their own each struggle in stochastic, high-dimensional domains, integrating them can overcome each of their weaknesses. This work was successful in carrying out complex robotics tasks, even with an inaccurate model, and moreover, it showed that the region of the environment that is explored by the agent is more reasonable and is goal-directed.