Efficient exploration is imperative to success in Reinforcement Learning. However, random approaches, which are not inefficient, are often favoured in practice due to their ease of implementation and robustness. Model-based approaches to exploration are typically more sample efficient, but much of the current approaches are either very optimistic or computationally expensive, and rely on learning a model entirely from scratch and assume that it will eventually become correct.
\newline\newline Within this work we explored a model-based approach to exploration that leverages an initial model, uses a \textit{reasonable} amount of optimism, alongside intrinsic motivation, to explore and learn whilst not assuming that the model will eventually become correct. Exploration was driven by enabling the planner to hypothesise where the model may be incorrect, through additional actions denoted \textit{Meta Actions} and then realising these hypotheses through experience.
\newline\newline Various implementations were developed based on this idea, and were evaluated in a collection of gridworld-like domains, and our method of exploration was shown to perform better than the most ubiquitous random method, $\epsilon$-greedy, in most cases; moreover, the agents with \textit{Meta Actions} available to them often outperformed those without them, showing that they can be a useful exploration mechanism.