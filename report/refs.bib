@article{AIJ16-leonetti,
title = "A synthesis of automated planning and reinforcement learning for 
efficient, robust decision-making",
journal = "Artificial Intelligence",
volume = "241",
pages = "103 - 130",
year = "2016",
month = "September",
issn = "0004-3702",
doi = "http://dx.doi.org/10.1016/j.artint.2016.07.004",
url = "http://www.sciencedirect.com/science/article/pii/S0004370216300819",
author = "Matteo Leonetti and Luca Iocchi and Peter Stone",
abstract = {Abstract Automated planning and reinforcement learning are 
characterized by complementary views on decision making: the former relies on 
previous knowledge and computation, while the latter on interaction with the 
world, and experience. Planning allows robots to carry out different tasks in 
the same domain, without the need to acquire knowledge about each one of them, 
but relies strongly on the accuracy of the model. Reinforcement learning, on the 
other hand, does not require previous knowledge, and allows robots to robustly 
adapt to the environment, but often necessitates an infeasible amount of 
experience. We present Domain Approximation for Reinforcement LearnING 
(DARLING), a method that takes advantage of planning to constrain the behavior 
of the agent to reasonable choices, and of reinforcement learning to adapt to 
the environment, and increase the reliability of the decision making process. We 
demonstrate the effectiveness of the proposed method on a service robot, 
carrying out a variety of tasks in an office building. We find that when the 
robot makes decisions by planning alone on a given model it often fails, and 
when it makes decisions by reinforcement learning alone it often cannot complete 
its tasks in a reasonable amount of time. When employing DARLING, even when 
seeded with the same model that was used for planning alone, however, the robot 
can quickly learn a behavior to carry out all the tasks, improves over time, and 
adapts to the environment as it changes.},
}

@Article{Sutton:1988,
  author =       "Sutton, Richard S.",
  title =        "Learning to Predict By the Methods of Temporal Differences",
  journal =      "Machine Learning",
  year =         "1988",
  volume =    "3",
  number =    "1",
  pages =     "9--44",
  month =     "August",
  publisher = "Springer Netherlands",
  bib2html_rescat = "General RL, Function Approximation",
}

@PhdThesis{Watkins:1989,
  author =       "Watkins, Christopher John Cornish Hellaby",
  title =        "Learning from Delayed Rewards",
  school =       "King's College",
  year =         "1989",
  address =   "Cambridge, UK",
  month =     "May",
  bib2html_rescat = "Parameter",
}

@book{PooleMackworth17,
  abstract = {Artificial intelligence, including machine learning, has emerged as a transformational science and engineering discipline. Artificial Intelligence: Foundations of Computational Agents presents AI using a coherent framework to study the design of intelligent computational agents. By showing how the basic approaches fit into a multidimensional design space, readers learn the fundamentals without losing sight of the bigger picture. The new edition also features expanded coverage on machine learning material, as well as on the social and ethical consequences of AI and ML. The book balances theory and experiment, showing how to link them together, and develops the science of AI together with its engineering applications. Although structured as an undergraduate and graduate textbook, the book's straightforward, self-contained style will also appeal to an audience of professionals, researchers, and independent learners. The second edition is well-supported by strong pedagogical features and online resources to enhance student comprehension.},
  added-at = {2017-10-22T13:53:07.000+0200},
  address = {Cambridge, UK},
  author = {Poole, David and Mackworth, Alan},
  biburl = {https://www.bibsonomy.org/bibtex/2e4ec397b0ebf6769d87584d7fd8cb6f4/flint63},
  edition = 2,
  file = {Excerpt (from DRM eBook):2017/PooleMackworth17intro.pdf:PDF;Fist Edition 2010:2010/PooleMackworth10.pdf:PDF;eBooks.com:https\://sec.ebooks.com/account/view-all.asp:URL;Cambridge University Press Product Page:http\://www.cambridge.org/9781107195394:URL;Amazon Search inside:http\://www.amazon.de/gp/reader/110719539X/:URL;Related Web Site:http\://aispace.org/:URL},
  groups = {public},
  interhash = {6a01b002d9efaab2dabb291411ccfd1b},
  intrahash = {e4ec397b0ebf6769d87584d7fd8cb6f4},
  isbn = {978-0-521-51900-7},
  keywords = {01801 102 book safari ai agent knowledge processing plan learn intro},
  publisher = {Cambridge University Press},
  timestamp = {2018-04-16T12:41:15.000+0200},
  title = {Artificial Intelligence: Foundations of Computational Agents},
  url = {http://artint.info/2e/html/ArtInt2e.html},
  username = {flint63},
  year = 2017
}

@book{DBLP:books/lib/SuttonB98,
  author    = {Richard S. Sutton and
               Andrew G. Barto},
  title     = {Reinforcement learning - an introduction},
  series    = {Adaptive computation and machine learning},
  publisher = {{MIT} Press},
  year      = {1998},
  url       = {https://www.worldcat.org/oclc/37293240},
  isbn      = {978-0-262-19398-6},
  timestamp = {Fri, 17 Jul 2020 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/books/lib/SuttonB98.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bams/1183519147,
author = {Richard Bellman},
title = {{The theory of dynamic programming}},
volume = {60},
journal = {Bulletin of the American Mathematical Society},
number = {6},
publisher = {American Mathematical Society},
pages = {503 -- 515},
year = {1954},
doi = {bams/1183519147},
URL = {https://doi.org/}
}

@book{russelNorvig2003:aima,
  abstract = {{The long-anticipated revision of this best-selling book offers the most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For those interested in artificial intelligence.}},
  added-at = {2010-04-19T01:35:48.000+0200},
  author = {Russell, Stuart J. and Norvig, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/2aab790cf69e18d11b87b7fdbb1a0bf9e/franz},
  citeulike-article-id = {113848},
  description = {import from cite-u-like},
  howpublished = {Hardcover},
  interhash = {2d5e32236d3ab0ad8a224c2de22fcbb7},
  intrahash = {aab790cf69e18d11b87b7fdbb1a0bf9e},
  isbn = {0137903952},
  keywords = {intelligence},
  month = {December},
  posted-at = {2007-12-04 01:39:10},
  priority = {0},
  publisher = {{Prentice Hall}},
  timestamp = {2010-04-19T01:35:48.000+0200},
  title = {Artificial Intelligence: A Modern Approach (2nd Edition)},
  url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0137903952},
  year = 2002
}

@ARTICLE{4082128,

  author={Hart, Peter E. and Nilsson, Nils J. and Raphael, Bertram},

  journal={IEEE Transactions on Systems Science and Cybernetics}, 

  title={A Formal Basis for the Heuristic Determination of Minimum Cost Paths}, 

  year={1968},

  volume={4},

  number={2},

  pages={100-107},

  doi={10.1109/TSSC.1968.300136}}

@techreport{Thrun-1992-15850,
author = {Sebastian Thrun},
title = {Efficient Exploration In Reinforcement Learning.},
year = {1992},
month = {January},
institution = {Carnegie Mellon University},
address = {Pittsburgh, PA},
number = {CMU-CS-92-102},
}

@article{DBLP:journals/corr/abs-2109-00157,
  author    = {Susan Amin and
               Maziar Gomrokchi and
               Harsh Satija and
               Herke van Hoof and
               Doina Precup},
  title     = {A Survey of Exploration Methods in Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/2109.00157},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.00157},
  eprinttype = {arXiv},
  eprint    = {2109.00157},
  timestamp = {Mon, 20 Sep 2021 16:29:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-00157.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{journals/ml/WatkinsD92,
  added-at = {2020-03-02T00:00:00.000+0100},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/236a8446d61a68f0378745309491b5ac1/dblp},
  ee = {https://www.wikidata.org/entity/Q57424214},
  interhash = {e3fd55c697f7cfd96be6789dc1b2c289},
  intrahash = {36a8446d61a68f0378745309491b5ac1},
  journal = {Mach. Learn.},
  keywords = {dblp},
  pages = {279-292},
  timestamp = {2020-03-03T11:49:53.000+0100},
  title = {Technical Note Q-Learning.},
  url = {http://dblp.uni-trier.de/db/journals/ml/ml8.html#WatkinD92},
  volume = 8,
  year = 1992
}

@Book{nla.cat-vn2770732,
author = { Skinner, B. F. },
title = { The behavior of organisms : and experimental analysis / by B. F. Skinner },
publisher = { Appleton-Century-Crofts New York },
pages = { xv, 457 p. : },
year = { 1938 },
type = { Book },
language = { English },
subjects = { Rats.; Psychophysiology. },
life-dates = {  - 1938 },
catalogue-url = { https://nla.gov.au/nla.cat-vn2770732 },
}

@inproceedings{NIPS2006_c1b70d96,
 author = {Auer, Peter and Ortner, Ronald},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
 pages = {},
 publisher = {MIT Press},
 title = {Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2006/file/c1b70d965ca504aa751ddb62ad69c63f-Paper.pdf},
 volume = {19},
 year = {2006}
}

@InProceedings{SARA07-jong,
title={Model-Based Exploration in Continuous State Spaces},
author={Nicholas K. Jong and Peter Stone},
booktitle={The Seventh Symposium on Abstraction, Reformulation, and Approximation},
month={July},
url="http://www.cs.utexas.edu/users/ai-lab?SARA07-jong",
year={2007}
}

@inproceedings{NIPS1991_e5f6ad6c,
 author = {Thrun, Sebastian B. and M\"{o}ller, Knut},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Active Exploration in Dynamic Environments},
 url = {https://proceedings.neurips.cc/paper/1991/file/e5f6ad6ce374177eef023bf5d0c018b6-Paper.pdf},
 volume = {4},
 year = {1991}
}

@inproceedings{10.5555/295240.295801,
author = {Dearden, Richard and Friedman, Nir and Russell, Stuart},
title = {Bayesian Q-Learning},
year = {1998},
isbn = {0262510987},
publisher = {American Association for Artificial Intelligence},
address = {USA},
abstract = {A central problem in learning in complex environments is balancing exploration of untested actions against exploitation of actions that are known to be good. The benefit of exploration can be estimated using the classical notion of Value of Information-the expected improvement in future decision quality that might arise from the information acquired by exploration. Estimating this quantity requires an assessment of the agent's uncertainty about its current value estimates for states. In this paper, we adopt a Bayesian approach to maintaining this uncertain information. We extend Watkins' Q-learning by maintaining and propagating probability distributions over the Q-values. These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation. We establish the convergence properties of our algorithm and show experimentally that it can exhibit substantial improvements over other well-known model-free exploration strategies.},
booktitle = {Proceedings of the Fifteenth National/Tenth Conference on Artificial Intelligence/Innovative Applications of Artificial Intelligence},
pages = {761–768},
numpages = {8},
location = {Madison, Wisconsin, USA},
series = {AAAI '98/IAAI '98}
}

@book{Lav06,
  author       = {S. M. LaValle},
  title        = {Planning Algorithms},
  publisher    = {Cambridge University Press},
  address      = {Cambridge, U.K.},
  note         = {Available at http://planning.cs.uiuc.edu/},
  year         = {2006}
}

@article{silver2017mastering,
  added-at = {2017-12-15T02:14:58.000+0100},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2ecdfbfcceb55ee5f14c1c375ad71f2cb/achakraborty},
  description = {Mastering the game of Go without human knowledge | Nature},
  interhash = {c45d318e105d0f2d62ccc28c2699d9d4},
  intrahash = {ecdfbfcceb55ee5f14c1c375ad71f2cb},
  journal = {Nature},
  keywords = {2017 deep-learning deepmind google paper reinforcement-learning},
  month = oct,
  pages = {354--},
  publisher = {Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  timestamp = {2017-12-15T02:14:58.000+0100},
  title = {Mastering the game of Go without human knowledge},
  url = {http://dx.doi.org/10.1038/nature24270},
  volume = 550,
  year = 2017
}

@InProceedings{pmlr-v28-levine13,
  title = 	 {Guided Policy Search},
  author = 	 {Levine, Sergey and Koltun, Vladlen},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1--9},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/levine13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/levine13.html},
  abstract = 	 {Direct policy search can effectively scale to high-dimensional systems, but complex policies with hundreds of parameters often present a challenge for such methods, requiring numerous samples and often falling into poor local optima. We present a guided policy search algorithm that uses trajectory optimization to direct policy learning and avoid poor local optima. We show how differential dynamic programming can be used to generate suitable guiding samples, and describe a regularized importance sampled policy optimization that incorporates these samples into the policy search. We evaluate the method by learning neural network controllers for planar swimming, hopping, and walking, as well as simulated 3D humanoid running.}
}

@article{MAL-086,
url = {http://dx.doi.org/10.1561/2200000086},
year = {2023},
volume = {16},
journal = {Foundations and Trends® in Machine Learning},
title = {Model-based Reinforcement Learning: A Survey},
doi = {10.1561/2200000086},
issn = {1935-8237},
number = {1},
pages = {1-118},
author = {Thomas M. Moerland and Joost Broekens and Aske Plaat and Catholijn M. Jonker}
}

@article{Minsky:1961:ire,
  abstract = {The problems of heuristic programming---of making computers solve
	really difficult problems---are divided into five main areas: Search,
	Pattern-Recognition, Learning, Planning, and Induction. Wherever
	appropriate, the discussion is supported by extensive citation of
	the literature and by descriptions of a few of the most successful
	heuristic (problem-solving) programs constructed to date.
	
	The adjective "heuristic," as used here and widely in the literature,
	means related to improving problem-solving performance; as a noun
	it is also used in regard to any method or trick used to improve
	the efficiency of a problem-solving system. A "heuristic program,"
	to be considered successful, must work well on a variety of problems,
	and may often be excused if it fails on some. We often find it worthwhile
	to introduce a heuristic method, which happens to cause occasional
	failures, if there is an over-all improvement in performance. But
	imperfect methods are not necessarily heuristic, nor vice versa.
	Hence "heuristic" should not be regarded as opposite to "foolproof";
	this has caused some confusion in the literature.},
  added-at = {2017-03-16T11:50:55.000+0100},
  author = {Minsky, Marvin},
  biburl = {https://www.bibsonomy.org/bibtex/2f1f2d84d704a03a03ef4e0282c44a6f3/krevelen},
  interhash = {e3ea30125d28343ae56ccc83d6e32cf9},
  intrahash = {f1f2d84d704a03a03ef4e0282c44a6f3},
  journal = {Proc. IRE},
  keywords = {imported thesis},
  month = jan,
  owner = {Rick},
  timestamp = {2017-03-16T11:54:14.000+0100},
  title = {Steps toward artificial intelligence},
  url = {http://web.media.mit.edu/~minsky/papers/steps.html},
  year = 1961
}

@phdthesis{10.5555/911176,
author = {Sutton, Richard Stuart},
title = {Temporal Credit Assignment in Reinforcement Learning},
year = {1984},
publisher = {University of Massachusetts Amherst},
abstract = {This dissertation describes computational experiments comparing the performance of a range of reinforcement-learning algorithms. The experiments are designed to focus on aspects of the credit-assignment problem having to do with determining when the behavior that deserves credit occurred. The issues of knowledge representation involved in developing new features or refining existing ones are not addressed. The algorithms considered include some from learning automata theory, mathematical learning theory, early "cybernetic" approaches to learning, Samuel's checker-playing program, Michie and Chambers's "Boxes" system, and a number of new algorithms. The tasks were selected so as to involve, first in isolation and then in combination, the issues of misleading generalizations, delayed reinforcement, unbalanced reinforcement, and secondary reinforcement. The tasks range from simple, abstract "two-armed bandit" tasks to a physically realistic pole-balancing task.The results indicate several areas where the algorithms presented here perform substantially better than those previously studied. An unbalanced distribution of reinforcement, misleading generalizations, and delayed reinforcement can greatly retard learning and in some cases even make it counterproductive. Performance can be substantially improved in the presence of these common problems through the use of mechanisms of reinforcement comparison and secondary reinforcement. We present a new algorithm similar to the "learning-by-generalization" algorithm used for altering the static evaluation function in Samuel's checker-playing program. Simulation experiments indicate that the new algorithm performs better than a version of Samuel's algorithm suitably modified for reinforcement learning tasks. Theoretical analysis in terms of an "ideal reinforcement signal" sheds light on the relationship between these two algorithms and other temporal credit-assignment algorithms.},
note = {AAI8410337}
}

@ARTICLE{5392560,

  author={Samuel, A. L.},

  journal={IBM Journal of Research and Development}, 

  title={Some Studies in Machine Learning Using the Game of Checkers}, 

  year={1959},

  volume={3},

  number={3},

  pages={210-229},

  doi={10.1147/rd.33.0210}}

@techreport{rummery:cuedtr94,
  added-at = {2008-03-11T14:52:34.000+0100},
  address = {Cambridge, England},
  author = {Rummery, G. A. and Niranjan, M.},
  biburl = {https://www.bibsonomy.org/bibtex/220239f82583859588c96171ccc015e65/idsia},
  citeulike-article-id = {2378892},
  institution = {Cambridge University Engineering Department},
  interhash = {0c7cd3821ad0fe1b39a6ce1b35ec4bc0},
  intrahash = {20239f82583859588c96171ccc015e65},
  keywords = {nn},
  number = {TR 166},
  priority = {2},
  timestamp = {2008-03-11T14:59:39.000+0100},
  title = {On-Line {Q}-Learning Using Connectionist Systems},
  year = 1994
}

@inproceedings{conf/nips/Sutton95,
  added-at = {2020-03-06T00:00:00.000+0100},
  author = {Sutton, Richard S.},
  biburl = {https://www.bibsonomy.org/bibtex/2633b671232afdadc63a137deca746be6/dblp},
  booktitle = {NIPS},
  editor = {Touretzky, David S. and Mozer, Michael and Hasselmo, Michael E.},
  ee = {http://nips.djvuzone.org/djvu/nips08/1038.djvu},
  interhash = {ff6b0036cbf1939b745a39f17c1cd667},
  intrahash = {633b671232afdadc63a137deca746be6},
  isbn = {0-262-20107-0},
  keywords = {dblp},
  pages = {1038-1044},
  publisher = {MIT Press},
  timestamp = {2020-03-07T11:46:07.000+0100},
  title = {Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding.},
  url = {http://dblp.uni-trier.de/db/conf/nips/nips1995.html#Sutton95},
  year = 1995
}

@inproceedings{
dabney2021temporallyextended,
title={Temporally-Extended {\ensuremath{\varepsilon}}-Greedy Exploration},
author={Will Dabney and Georg Ostrovski and Andre Barreto},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=ONBPHFZ7zG4}
}

@InProceedings{10.1007/978-3-642-76153-9_28,
author="Bridle, John S.",
editor="Souli{\'e}, Fran{\c{c}}oise Fogelman
and H{\'e}rault, Jeanny",
title="Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition",
booktitle="Neurocomputing",
year="1990",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="227--236",
abstract="We are concerned with feed-forward non-linear networks (multi-layer perceptrons, or MLPs) with multiple outputs. We wish to treat the outputs of the network as probabilities of alternatives (e.g. pattern classes), conditioned on the inputs. We look for appropriate output non-linearities and for appropriate criteria for adaptation of the parameters of the network (e.g. weights). We explain two modifications: probability scoring, which is an alternative to squared error minimisation, and a normalised exponential (softmax) multi-input generalisation of the logistic non-linearity. The two modifications together result in quite simple arithmetic, and hardware implementation is not difficult either. The use of radial units (squared distance instead of dot product) immediately before the softmax output stage produces a network which computes posterior distributions over class labels based on an assumption of Gaussian within-class distributions. However the training, which uses cross-class information, can result in better performance at class discrimination than the usual within-class training method, unless the within-class distribution assumptions are actually correct.",
isbn="978-3-642-76153-9"
}

@phdthesis{anderson86,
author = {Anderson, Charles William},
year = {1986},
title = {Learning and Problem Solving with Multilayer Connectionist Systems}
}



@inproceedings{NIPS1989_17000029,
 author = {Mozer, Michael C and Bachrach, Jonathan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Discovering the Structure of a Reactive Environment by Exploration},
 url = {https://proceedings.neurips.cc/paper/1989/file/1700002963a49da13542e0726b7bb758-Paper.pdf},
 volume = {2},
 year = {1989}
}

