@article{AIJ16-leonetti,
title = "A synthesis of automated planning and reinforcement learning for 
efficient, robust decision-making",
journal = "Artificial Intelligence",
volume = "241",
pages = "103 - 130",
year = "2016",
month = "September",
issn = "0004-3702",
doi = "http://dx.doi.org/10.1016/j.artint.2016.07.004",
url = "http://www.sciencedirect.com/science/article/pii/S0004370216300819",
author = "Matteo Leonetti and Luca Iocchi and Peter Stone",
abstract = {Abstract Automated planning and reinforcement learning are 
characterized by complementary views on decision making: the former relies on 
previous knowledge and computation, while the latter on interaction with the 
world, and experience. Planning allows robots to carry out different tasks in 
the same domain, without the need to acquire knowledge about each one of them, 
but relies strongly on the accuracy of the model. Reinforcement learning, on the 
other hand, does not require previous knowledge, and allows robots to robustly 
adapt to the environment, but often necessitates an infeasible amount of 
experience. We present Domain Approximation for Reinforcement LearnING 
(DARLING), a method that takes advantage of planning to constrain the behavior 
of the agent to reasonable choices, and of reinforcement learning to adapt to 
the environment, and increase the reliability of the decision making process. We 
demonstrate the effectiveness of the proposed method on a service robot, 
carrying out a variety of tasks in an office building. We find that when the 
robot makes decisions by planning alone on a given model it often fails, and 
when it makes decisions by reinforcement learning alone it often cannot complete 
its tasks in a reasonable amount of time. When employing DARLING, even when 
seeded with the same model that was used for planning alone, however, the robot 
can quickly learn a behavior to carry out all the tasks, improves over time, and 
adapts to the environment as it changes.},
}

@Article{Sutton:1988,
  author =       "Sutton, Richard S.",
  title =        "Learning to Predict By the Methods of Temporal Differences",
  journal =      "Machine Learning",
  year =         "1988",
  volume =    "3",
  number =    "1",
  pages =     "9--44",
  month =     "August",
  publisher = "Springer Netherlands",
  bib2html_rescat = "General RL, Function Approximation",
}

@PhdThesis{Watkins:1989,
  author =       "Watkins, Christopher John Cornish Hellaby",
  title =        "Learning from Delayed Rewards",
  school =       "King's College",
  year =         "1989",
  address =   "Cambridge, UK",
  month =     "May",
  bib2html_rescat = "Parameter",
}

@book{PooleMackworth17,
  abstract = {Artificial intelligence, including machine learning, has emerged as a transformational science and engineering discipline. Artificial Intelligence: Foundations of Computational Agents presents AI using a coherent framework to study the design of intelligent computational agents. By showing how the basic approaches fit into a multidimensional design space, readers learn the fundamentals without losing sight of the bigger picture. The new edition also features expanded coverage on machine learning material, as well as on the social and ethical consequences of AI and ML. The book balances theory and experiment, showing how to link them together, and develops the science of AI together with its engineering applications. Although structured as an undergraduate and graduate textbook, the book's straightforward, self-contained style will also appeal to an audience of professionals, researchers, and independent learners. The second edition is well-supported by strong pedagogical features and online resources to enhance student comprehension.},
  added-at = {2017-10-22T13:53:07.000+0200},
  address = {Cambridge, UK},
  author = {Poole, David and Mackworth, Alan},
  biburl = {https://www.bibsonomy.org/bibtex/2e4ec397b0ebf6769d87584d7fd8cb6f4/flint63},
  edition = 2,
  file = {Excerpt (from DRM eBook):2017/PooleMackworth17intro.pdf:PDF;Fist Edition 2010:2010/PooleMackworth10.pdf:PDF;eBooks.com:https\://sec.ebooks.com/account/view-all.asp:URL;Cambridge University Press Product Page:http\://www.cambridge.org/9781107195394:URL;Amazon Search inside:http\://www.amazon.de/gp/reader/110719539X/:URL;Related Web Site:http\://aispace.org/:URL},
  groups = {public},
  interhash = {6a01b002d9efaab2dabb291411ccfd1b},
  intrahash = {e4ec397b0ebf6769d87584d7fd8cb6f4},
  isbn = {978-0-521-51900-7},
  keywords = {01801 102 book safari ai agent knowledge processing plan learn intro},
  publisher = {Cambridge University Press},
  timestamp = {2018-04-16T12:41:15.000+0200},
  title = {Artificial Intelligence: Foundations of Computational Agents},
  url = {http://artint.info/2e/html/ArtInt2e.html},
  username = {flint63},
  year = 2017
}

@book{DBLP:books/lib/SuttonB98,
  author    = {Richard S. Sutton and
               Andrew G. Barto},
  title     = {Reinforcement learning - an introduction},
  series    = {Adaptive computation and machine learning},
  publisher = {{MIT} Press},
  year      = {1998},
  url       = {https://www.worldcat.org/oclc/37293240},
  isbn      = {978-0-262-19398-6},
  timestamp = {Fri, 17 Jul 2020 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/books/lib/SuttonB98.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{bams/1183519147,
author = {Richard Bellman},
title = {{The theory of dynamic programming}},
volume = {60},
journal = {Bulletin of the American Mathematical Society},
number = {6},
publisher = {American Mathematical Society},
pages = {503 -- 515},
year = {1954},
doi = {bams/1183519147},
URL = {https://doi.org/}
}

@book{russelNorvig2003:aima,
  abstract = {{The long-anticipated revision of this best-selling book offers the most comprehensive, up-to-date introduction to the theory and practice of artificial intelligence. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For those interested in artificial intelligence.}},
  added-at = {2010-04-19T01:35:48.000+0200},
  author = {Russell, Stuart J. and Norvig, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/2aab790cf69e18d11b87b7fdbb1a0bf9e/franz},
  citeulike-article-id = {113848},
  description = {import from cite-u-like},
  howpublished = {Hardcover},
  interhash = {2d5e32236d3ab0ad8a224c2de22fcbb7},
  intrahash = {aab790cf69e18d11b87b7fdbb1a0bf9e},
  isbn = {0137903952},
  keywords = {intelligence},
  month = {December},
  posted-at = {2007-12-04 01:39:10},
  priority = {0},
  publisher = {{Prentice Hall}},
  timestamp = {2010-04-19T01:35:48.000+0200},
  title = {Artificial Intelligence: A Modern Approach (2nd Edition)},
  url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0137903952},
  year = 2002
}

@ARTICLE{4082128,

  author={Hart, Peter E. and Nilsson, Nils J. and Raphael, Bertram},

  journal={IEEE Transactions on Systems Science and Cybernetics}, 

  title={A Formal Basis for the Heuristic Determination of Minimum Cost Paths}, 

  year={1968},

  volume={4},

  number={2},

  pages={100-107},

  doi={10.1109/TSSC.1968.300136}}

@techreport{Thrun-1992-15850,
author = {Sebastian Thrun},
title = {Efficient Exploration In Reinforcement Learning.},
year = {1992},
month = {January},
institution = {Carnegie Mellon University},
address = {Pittsburgh, PA},
number = {CMU-CS-92-102},
}

@article{DBLP:journals/corr/abs-2109-00157,
  author    = {Susan Amin and
               Maziar Gomrokchi and
               Harsh Satija and
               Herke van Hoof and
               Doina Precup},
  title     = {A Survey of Exploration Methods in Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/2109.00157},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.00157},
  eprinttype = {arXiv},
  eprint    = {2109.00157},
  timestamp = {Mon, 20 Sep 2021 16:29:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-00157.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{journals/ml/WatkinsD92,
  added-at = {2020-03-02T00:00:00.000+0100},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/236a8446d61a68f0378745309491b5ac1/dblp},
  ee = {https://www.wikidata.org/entity/Q57424214},
  interhash = {e3fd55c697f7cfd96be6789dc1b2c289},
  intrahash = {36a8446d61a68f0378745309491b5ac1},
  journal = {Mach. Learn.},
  keywords = {dblp},
  pages = {279-292},
  timestamp = {2020-03-03T11:49:53.000+0100},
  title = {Technical Note Q-Learning.},
  url = {http://dblp.uni-trier.de/db/journals/ml/ml8.html#WatkinD92},
  volume = 8,
  year = 1992
}

@Book{nla.cat-vn2770732,
author = { Skinner, B. F. },
title = { The behavior of organisms : and experimental analysis / by B. F. Skinner },
publisher = { Appleton-Century-Crofts New York },
pages = { xv, 457 p. : },
year = { 1938 },
type = { Book },
language = { English },
subjects = { Rats.; Psychophysiology. },
life-dates = {  - 1938 },
catalogue-url = { https://nla.gov.au/nla.cat-vn2770732 },
}

@inproceedings{NIPS2006_c1b70d96,
 author = {Auer, Peter and Ortner, Ronald},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
 pages = {},
 publisher = {MIT Press},
 title = {Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2006/file/c1b70d965ca504aa751ddb62ad69c63f-Paper.pdf},
 volume = {19},
 year = {2006}
}

@InProceedings{SARA07-jong,
title={Model-Based Exploration in Continuous State Spaces},
author={Nicholas K. Jong and Peter Stone},
booktitle={The Seventh Symposium on Abstraction, Reformulation, and Approximation},
month={July},
url="http://www.cs.utexas.edu/users/ai-lab?SARA07-jong",
year={2007}
}

@inproceedings{NIPS1991_e5f6ad6c,
 author = {Thrun, Sebastian B. and M\"{o}ller, Knut},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Active Exploration in Dynamic Environments},
 url = {https://proceedings.neurips.cc/paper/1991/file/e5f6ad6ce374177eef023bf5d0c018b6-Paper.pdf},
 volume = {4},
 year = {1991}
}

@inproceedings{10.5555/295240.295801,
author = {Dearden, Richard and Friedman, Nir and Russell, Stuart},
title = {Bayesian Q-Learning},
year = {1998},
isbn = {0262510987},
publisher = {American Association for Artificial Intelligence},
address = {USA},
abstract = {A central problem in learning in complex environments is balancing exploration of untested actions against exploitation of actions that are known to be good. The benefit of exploration can be estimated using the classical notion of Value of Information-the expected improvement in future decision quality that might arise from the information acquired by exploration. Estimating this quantity requires an assessment of the agent's uncertainty about its current value estimates for states. In this paper, we adopt a Bayesian approach to maintaining this uncertain information. We extend Watkins' Q-learning by maintaining and propagating probability distributions over the Q-values. These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation. We establish the convergence properties of our algorithm and show experimentally that it can exhibit substantial improvements over other well-known model-free exploration strategies.},
booktitle = {Proceedings of the Fifteenth National/Tenth Conference on Artificial Intelligence/Innovative Applications of Artificial Intelligence},
pages = {761–768},
numpages = {8},
location = {Madison, Wisconsin, USA},
series = {AAAI '98/IAAI '98}
}

@book{Lav06,
  author       = {S. M. LaValle},
  title        = {Planning Algorithms},
  publisher    = {Cambridge University Press},
  address      = {Cambridge, U.K.},
  note         = {Available at http://planning.cs.uiuc.edu/},
  year         = {2006}
}

@article{silver2017mastering,
  added-at = {2017-12-15T02:14:58.000+0100},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2ecdfbfcceb55ee5f14c1c375ad71f2cb/achakraborty},
  description = {Mastering the game of Go without human knowledge | Nature},
  interhash = {c45d318e105d0f2d62ccc28c2699d9d4},
  intrahash = {ecdfbfcceb55ee5f14c1c375ad71f2cb},
  journal = {Nature},
  keywords = {2017 deep-learning deepmind google paper reinforcement-learning},
  month = oct,
  pages = {354--},
  publisher = {Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  timestamp = {2017-12-15T02:14:58.000+0100},
  title = {Mastering the game of Go without human knowledge},
  url = {http://dx.doi.org/10.1038/nature24270},
  volume = 550,
  year = 2017
}

@InProceedings{pmlr-v28-levine13,
  title = 	 {Guided Policy Search},
  author = 	 {Levine, Sergey and Koltun, Vladlen},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1--9},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/levine13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/levine13.html},
  abstract = 	 {Direct policy search can effectively scale to high-dimensional systems, but complex policies with hundreds of parameters often present a challenge for such methods, requiring numerous samples and often falling into poor local optima. We present a guided policy search algorithm that uses trajectory optimization to direct policy learning and avoid poor local optima. We show how differential dynamic programming can be used to generate suitable guiding samples, and describe a regularized importance sampled policy optimization that incorporates these samples into the policy search. We evaluate the method by learning neural network controllers for planar swimming, hopping, and walking, as well as simulated 3D humanoid running.}
}

@article{MAL-086,
url = {http://dx.doi.org/10.1561/2200000086},
year = {2023},
volume = {16},
journal = {Foundations and Trends® in Machine Learning},
title = {Model-based Reinforcement Learning: A Survey},
doi = {10.1561/2200000086},
issn = {1935-8237},
number = {1},
pages = {1-118},
author = {Thomas M. Moerland and Joost Broekens and Aske Plaat and Catholijn M. Jonker}
}