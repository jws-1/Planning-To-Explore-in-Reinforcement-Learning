@article{AIJ16-leonetti,
title = "A synthesis of automated planning and reinforcement learning for 
efficient, robust decision-making",
journal = "Artificial Intelligence",
volume = "241",
pages = "103 - 130",
year = "2016",
month = "September",
issn = "0004-3702",
doi = "http://dx.doi.org/10.1016/j.artint.2016.07.004",
url = "http://www.sciencedirect.com/science/article/pii/S0004370216300819",
author = "Matteo Leonetti and Luca Iocchi and Peter Stone",
abstract = {Abstract Automated planning and reinforcement learning are 
characterized by complementary views on decision making: the former relies on 
previous knowledge and computation, while the latter on interaction with the 
world, and experience. Planning allows robots to carry out different tasks in 
the same domain, without the need to acquire knowledge about each one of them, 
but relies strongly on the accuracy of the model. Reinforcement learning, on the 
other hand, does not require previous knowledge, and allows robots to robustly 
adapt to the environment, but often necessitates an infeasible amount of 
experience. We present Domain Approximation for Reinforcement LearnING 
(DARLING), a method that takes advantage of planning to constrain the behavior 
of the agent to reasonable choices, and of reinforcement learning to adapt to 
the environment, and increase the reliability of the decision making process. We 
demonstrate the effectiveness of the proposed method on a service robot, 
carrying out a variety of tasks in an office building. We find that when the 
robot makes decisions by planning alone on a given model it often fails, and 
when it makes decisions by reinforcement learning alone it often cannot complete 
its tasks in a reasonable amount of time. When employing DARLING, even when 
seeded with the same model that was used for planning alone, however, the robot 
can quickly learn a behavior to carry out all the tasks, improves over time, and 
adapts to the environment as it changes.},
}

@Article{Sutton:1988,
  author =       "Sutton, Richard S.",
  title =        "Learning to Predict By the Methods of Temporal Differences",
  journal =      "Machine Learning",
  year =         "1988",
  volume =    "3",
  number =    "1",
  pages =     "9--44",
  month =     "August",
  publisher = "Springer Netherlands",
  bib2html_rescat = "General RL, Function Approximation",
}

@PhdThesis{Watkins:1989,
  author =       "Watkins, Christopher John Cornish Hellaby",
  title =        "Learning from Delayed Rewards",
  school =       "King's College",
  year =         "1989",
  address =   "Cambridge, UK",
  month =     "May",
  bib2html_rescat = "Parameter",
}

@book{PooleMackworth17,
  abstract = {Artificial intelligence, including machine learning, has emerged as a transformational science and engineering discipline. Artificial Intelligence: Foundations of Computational Agents presents AI using a coherent framework to study the design of intelligent computational agents. By showing how the basic approaches fit into a multidimensional design space, readers learn the fundamentals without losing sight of the bigger picture. The new edition also features expanded coverage on machine learning material, as well as on the social and ethical consequences of AI and ML. The book balances theory and experiment, showing how to link them together, and develops the science of AI together with its engineering applications. Although structured as an undergraduate and graduate textbook, the book's straightforward, self-contained style will also appeal to an audience of professionals, researchers, and independent learners. The second edition is well-supported by strong pedagogical features and online resources to enhance student comprehension.},
  added-at = {2017-10-22T13:53:07.000+0200},
  address = {Cambridge, UK},
  author = {Poole, David and Mackworth, Alan},
  biburl = {https://www.bibsonomy.org/bibtex/2e4ec397b0ebf6769d87584d7fd8cb6f4/flint63},
  edition = 2,
  file = {Excerpt (from DRM eBook):2017/PooleMackworth17intro.pdf:PDF;Fist Edition 2010:2010/PooleMackworth10.pdf:PDF;eBooks.com:https\://sec.ebooks.com/account/view-all.asp:URL;Cambridge University Press Product Page:http\://www.cambridge.org/9781107195394:URL;Amazon Search inside:http\://www.amazon.de/gp/reader/110719539X/:URL;Related Web Site:http\://aispace.org/:URL},
  groups = {public},
  interhash = {6a01b002d9efaab2dabb291411ccfd1b},
  intrahash = {e4ec397b0ebf6769d87584d7fd8cb6f4},
  isbn = {978-0-521-51900-7},
  keywords = {01801 102 book safari ai agent knowledge processing plan learn intro},
  publisher = {Cambridge University Press},
  timestamp = {2018-04-16T12:41:15.000+0200},
  title = {Artificial Intelligence: Foundations of Computational Agents},
  url = {http://artint.info/2e/html/ArtInt2e.html},
  username = {flint63},
  year = 2017
}

@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}

@article{bams/1183519147,
author = {Richard Bellman},
title = {{The theory of dynamic programming}},
volume = {60},
journal = {Bulletin of the American Mathematical Society},
number = {6},
publisher = {American Mathematical Society},
pages = {503 -- 515},
year = {1954},
doi = {bams/1183519147},
URL = {https://doi.org/}
}

@book{DBLP:books/aw/RN2020,
  author    = {Stuart Russell and
               Peter Norvig},
  title     = {Artificial Intelligence: {A} Modern Approach (4th Edition)},
  publisher = {Pearson},
  year      = {2020},
  url       = {http://aima.cs.berkeley.edu/},
  isbn      = {9780134610993},
  timestamp = {Wed, 20 Apr 2022 13:29:51 +0200},
  biburl    = {https://dblp.org/rec/books/aw/RN2020.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{4082128,

  author={Hart, Peter E. and Nilsson, Nils J. and Raphael, Bertram},

  journal={IEEE Transactions on Systems Science and Cybernetics}, 

  title={A Formal Basis for the Heuristic Determination of Minimum Cost Paths}, 

  year={1968},

  volume={4},

  number={2},

  pages={100-107},

  doi={10.1109/TSSC.1968.300136}}

@techreport{Thrun-1992-15850,
author = {Sebastian Thrun},
title = {Efficient Exploration In Reinforcement Learning.},
year = {1992},
month = {January},
institution = {Carnegie Mellon University},
address = {Pittsburgh, PA},
number = {CMU-CS-92-102},
}

@article{DBLP:journals/corr/abs-2109-00157,
  author    = {Susan Amin and
               Maziar Gomrokchi and
               Harsh Satija and
               Herke van Hoof and
               Doina Precup},
  title     = {A Survey of Exploration Methods in Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/2109.00157},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.00157},
  eprinttype = {arXiv},
  eprint    = {2109.00157},
  timestamp = {Mon, 20 Sep 2021 16:29:41 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-00157.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{journals/ml/WatkinsD92,
  added-at = {2020-03-02T00:00:00.000+0100},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  biburl = {https://www.bibsonomy.org/bibtex/236a8446d61a68f0378745309491b5ac1/dblp},
  ee = {https://www.wikidata.org/entity/Q57424214},
  interhash = {e3fd55c697f7cfd96be6789dc1b2c289},
  intrahash = {36a8446d61a68f0378745309491b5ac1},
  journal = {Mach. Learn.},
  keywords = {dblp},
  pages = {279-292},
  timestamp = {2020-03-03T11:49:53.000+0100},
  title = {Technical Note Q-Learning.},
  url = {http://dblp.uni-trier.de/db/journals/ml/ml8.html#WatkinD92},
  volume = 8,
  year = 1992
}

@Book{nla.cat-vn2770732,
author = { Skinner, B. F. },
title = { The behavior of organisms : and experimental analysis / by B. F. Skinner },
publisher = { Appleton-Century-Crofts New York },
pages = { xv, 457 p. : },
year = { 1938 },
type = { Book },
language = { English },
subjects = { Rats.; Psychophysiology. },
life-dates = {  - 1938 },
catalogue-url = { https://nla.gov.au/nla.cat-vn2770732 },
}

@inproceedings{NIPS2006_c1b70d96,
 author = {Auer, Peter and Ortner, Ronald},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
 pages = {},
 publisher = {MIT Press},
 title = {Logarithmic Online Regret Bounds for Undiscounted Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/2006/file/c1b70d965ca504aa751ddb62ad69c63f-Paper.pdf},
 volume = {19},
 year = {2006}
}

@InProceedings{SARA07-jong,
title={Model-Based Exploration in Continuous State Spaces},
author={Nicholas K. Jong and Peter Stone},
booktitle={The Seventh Symposium on Abstraction, Reformulation, and Approximation},
month={July},
url="http://www.cs.utexas.edu/users/ai-lab?SARA07-jong",
year={2007}
}

@inproceedings{NIPS1991_e5f6ad6c,
 author = {Thrun, Sebastian B. and M\"{o}ller, Knut},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Moody and S. Hanson and R.P. Lippmann},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Active Exploration in Dynamic Environments},
 url = {https://proceedings.neurips.cc/paper/1991/file/e5f6ad6ce374177eef023bf5d0c018b6-Paper.pdf},
 volume = {4},
 year = {1991}
}

@inproceedings{10.5555/295240.295801,
author = {Dearden, Richard and Friedman, Nir and Russell, Stuart},
title = {Bayesian Q-Learning},
year = {1998},
isbn = {0262510987},
publisher = {American Association for Artificial Intelligence},
address = {USA},
abstract = {A central problem in learning in complex environments is balancing exploration of untested actions against exploitation of actions that are known to be good. The benefit of exploration can be estimated using the classical notion of Value of Information-the expected improvement in future decision quality that might arise from the information acquired by exploration. Estimating this quantity requires an assessment of the agent's uncertainty about its current value estimates for states. In this paper, we adopt a Bayesian approach to maintaining this uncertain information. We extend Watkins' Q-learning by maintaining and propagating probability distributions over the Q-values. These distributions are used to compute a myopic approximation to the value of information for each action and hence to select the action that best balances exploration and exploitation. We establish the convergence properties of our algorithm and show experimentally that it can exhibit substantial improvements over other well-known model-free exploration strategies.},
booktitle = {Proceedings of the Fifteenth National/Tenth Conference on Artificial Intelligence/Innovative Applications of Artificial Intelligence},
pages = {761–768},
numpages = {8},
location = {Madison, Wisconsin, USA},
series = {AAAI '98/IAAI '98}
}

@book{Lav06,
  author       = {S. M. LaValle},
  title        = {Planning Algorithms},
  publisher    = {Cambridge University Press},
  address      = {Cambridge, U.K.},
  note         = {Available at http://planning.cs.uiuc.edu/},
  year         = {2006}
}

@article{silver2017mastering,
  added-at = {2017-12-15T02:14:58.000+0100},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2ecdfbfcceb55ee5f14c1c375ad71f2cb/achakraborty},
  description = {Mastering the game of Go without human knowledge | Nature},
  interhash = {c45d318e105d0f2d62ccc28c2699d9d4},
  intrahash = {ecdfbfcceb55ee5f14c1c375ad71f2cb},
  journal = {Nature},
  keywords = {2017 deep-learning deepmind google paper reinforcement-learning},
  month = oct,
  pages = {354--},
  publisher = {Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  timestamp = {2017-12-15T02:14:58.000+0100},
  title = {Mastering the game of Go without human knowledge},
  url = {http://dx.doi.org/10.1038/nature24270},
  volume = 550,
  year = 2017
}

@InProceedings{pmlr-v28-levine13,
  title = 	 {Guided Policy Search},
  author = 	 {Levine, Sergey and Koltun, Vladlen},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1--9},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/levine13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/levine13.html},
  abstract = 	 {Direct policy search can effectively scale to high-dimensional systems, but complex policies with hundreds of parameters often present a challenge for such methods, requiring numerous samples and often falling into poor local optima. We present a guided policy search algorithm that uses trajectory optimization to direct policy learning and avoid poor local optima. We show how differential dynamic programming can be used to generate suitable guiding samples, and describe a regularized importance sampled policy optimization that incorporates these samples into the policy search. We evaluate the method by learning neural network controllers for planar swimming, hopping, and walking, as well as simulated 3D humanoid running.}
}

@article{MAL-086,
url = {http://dx.doi.org/10.1561/2200000086},
year = {2023},
volume = {16},
journal = {Foundations and Trends® in Machine Learning},
title = {Model-based Reinforcement Learning: A Survey},
doi = {10.1561/2200000086},
issn = {1935-8237},
number = {1},
pages = {1-118},
author = {Thomas M. Moerland and Joost Broekens and Aske Plaat and Catholijn M. Jonker}
}

@article{Minsky:1961:ire,
  abstract = {The problems of heuristic programming---of making computers solve
	really difficult problems---are divided into five main areas: Search,
	Pattern-Recognition, Learning, Planning, and Induction. Wherever
	appropriate, the discussion is supported by extensive citation of
	the literature and by descriptions of a few of the most successful
	heuristic (problem-solving) programs constructed to date.
	
	The adjective "heuristic," as used here and widely in the literature,
	means related to improving problem-solving performance; as a noun
	it is also used in regard to any method or trick used to improve
	the efficiency of a problem-solving system. A "heuristic program,"
	to be considered successful, must work well on a variety of problems,
	and may often be excused if it fails on some. We often find it worthwhile
	to introduce a heuristic method, which happens to cause occasional
	failures, if there is an over-all improvement in performance. But
	imperfect methods are not necessarily heuristic, nor vice versa.
	Hence "heuristic" should not be regarded as opposite to "foolproof";
	this has caused some confusion in the literature.},
  added-at = {2017-03-16T11:50:55.000+0100},
  author = {Minsky, Marvin},
  biburl = {https://www.bibsonomy.org/bibtex/2f1f2d84d704a03a03ef4e0282c44a6f3/krevelen},
  interhash = {e3ea30125d28343ae56ccc83d6e32cf9},
  intrahash = {f1f2d84d704a03a03ef4e0282c44a6f3},
  journal = {Proc. IRE},
  keywords = {imported thesis},
  month = jan,
  owner = {Rick},
  timestamp = {2017-03-16T11:54:14.000+0100},
  title = {Steps toward artificial intelligence},
  url = {http://web.media.mit.edu/~minsky/papers/steps.html},
  year = 1961
}

@phdthesis{10.5555/911176,
author = {Sutton, Richard Stuart},
title = {Temporal Credit Assignment in Reinforcement Learning},
year = {1984},
publisher = {University of Massachusetts Amherst},
abstract = {This dissertation describes computational experiments comparing the performance of a range of reinforcement-learning algorithms. The experiments are designed to focus on aspects of the credit-assignment problem having to do with determining when the behavior that deserves credit occurred. The issues of knowledge representation involved in developing new features or refining existing ones are not addressed. The algorithms considered include some from learning automata theory, mathematical learning theory, early "cybernetic" approaches to learning, Samuel's checker-playing program, Michie and Chambers's "Boxes" system, and a number of new algorithms. The tasks were selected so as to involve, first in isolation and then in combination, the issues of misleading generalizations, delayed reinforcement, unbalanced reinforcement, and secondary reinforcement. The tasks range from simple, abstract "two-armed bandit" tasks to a physically realistic pole-balancing task.The results indicate several areas where the algorithms presented here perform substantially better than those previously studied. An unbalanced distribution of reinforcement, misleading generalizations, and delayed reinforcement can greatly retard learning and in some cases even make it counterproductive. Performance can be substantially improved in the presence of these common problems through the use of mechanisms of reinforcement comparison and secondary reinforcement. We present a new algorithm similar to the "learning-by-generalization" algorithm used for altering the static evaluation function in Samuel's checker-playing program. Simulation experiments indicate that the new algorithm performs better than a version of Samuel's algorithm suitably modified for reinforcement learning tasks. Theoretical analysis in terms of an "ideal reinforcement signal" sheds light on the relationship between these two algorithms and other temporal credit-assignment algorithms.},
note = {AAI8410337}
}

@ARTICLE{5392560,

  author={Samuel, A. L.},

  journal={IBM Journal of Research and Development}, 

  title={Some Studies in Machine Learning Using the Game of Checkers}, 

  year={1959},

  volume={3},

  number={3},

  pages={210-229},

  doi={10.1147/rd.33.0210}}

@techreport{rummery:cuedtr94,
  added-at = {2008-03-11T14:52:34.000+0100},
  address = {Cambridge, England},
  author = {Rummery, G. A. and Niranjan, M.},
  biburl = {https://www.bibsonomy.org/bibtex/220239f82583859588c96171ccc015e65/idsia},
  citeulike-article-id = {2378892},
  institution = {Cambridge University Engineering Department},
  interhash = {0c7cd3821ad0fe1b39a6ce1b35ec4bc0},
  intrahash = {20239f82583859588c96171ccc015e65},
  keywords = {nn},
  number = {TR 166},
  priority = {2},
  timestamp = {2008-03-11T14:59:39.000+0100},
  title = {On-Line {Q}-Learning Using Connectionist Systems},
  year = 1994
}

@inproceedings{conf/nips/Sutton95,
  added-at = {2020-03-06T00:00:00.000+0100},
  author = {Sutton, Richard S.},
  biburl = {https://www.bibsonomy.org/bibtex/2633b671232afdadc63a137deca746be6/dblp},
  booktitle = {NIPS},
  editor = {Touretzky, David S. and Mozer, Michael and Hasselmo, Michael E.},
  ee = {http://nips.djvuzone.org/djvu/nips08/1038.djvu},
  interhash = {ff6b0036cbf1939b745a39f17c1cd667},
  intrahash = {633b671232afdadc63a137deca746be6},
  isbn = {0-262-20107-0},
  keywords = {dblp},
  pages = {1038-1044},
  publisher = {MIT Press},
  timestamp = {2020-03-07T11:46:07.000+0100},
  title = {Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding.},
  url = {http://dblp.uni-trier.de/db/conf/nips/nips1995.html#Sutton95},
  year = 1995
}

@inproceedings{
dabney2021temporallyextended,
title={Temporally-Extended {\ensuremath{\varepsilon}}-Greedy Exploration},
author={Will Dabney and Georg Ostrovski and Andre Barreto},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=ONBPHFZ7zG4}
}

@InProceedings{10.1007/978-3-642-76153-9_28,
author="Bridle, John S.",
editor="Souli{\'e}, Fran{\c{c}}oise Fogelman
and H{\'e}rault, Jeanny",
title="Probabilistic Interpretation of Feedforward Classification Network Outputs, with Relationships to Statistical Pattern Recognition",
booktitle="Neurocomputing",
year="1990",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="227--236",
abstract="We are concerned with feed-forward non-linear networks (multi-layer perceptrons, or MLPs) with multiple outputs. We wish to treat the outputs of the network as probabilities of alternatives (e.g. pattern classes), conditioned on the inputs. We look for appropriate output non-linearities and for appropriate criteria for adaptation of the parameters of the network (e.g. weights). We explain two modifications: probability scoring, which is an alternative to squared error minimisation, and a normalised exponential (softmax) multi-input generalisation of the logistic non-linearity. The two modifications together result in quite simple arithmetic, and hardware implementation is not difficult either. The use of radial units (squared distance instead of dot product) immediately before the softmax output stage produces a network which computes posterior distributions over class labels based on an assumption of Gaussian within-class distributions. However the training, which uses cross-class information, can result in better performance at class discrimination than the usual within-class training method, unless the within-class distribution assumptions are actually correct.",
isbn="978-3-642-76153-9"
}

@phdthesis{anderson86,
author = {Anderson, Charles William},
year = {1986},
title = {Learning and Problem Solving with Multilayer Connectionist Systems}
}



@inproceedings{NIPS1989_17000029,
 author = {Mozer, Michael C and Bachrach, Jonathan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Touretzky},
 pages = {},
 publisher = {Morgan-Kaufmann},
 title = {Discovering the Structure of a Reactive Environment by Exploration},
 url = {https://proceedings.neurips.cc/paper/1989/file/1700002963a49da13542e0726b7bb758-Paper.pdf},
 volume = {2},
 year = {1989}
}

@article{DBLP:journals/corr/PathakAED17,
  author    = {Deepak Pathak and
               Pulkit Agrawal and
               Alexei A. Efros and
               Trevor Darrell},
  title     = {Curiosity-driven Exploration by Self-supervised Prediction},
  journal   = {CoRR},
  volume    = {abs/1705.05363},
  year      = {2017},
  url       = {http://arxiv.org/abs/1705.05363},
  eprinttype = {arXiv},
  eprint    = {1705.05363},
  timestamp = {Mon, 13 Aug 2018 16:48:59 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/PathakAED17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{10.5555/944919.944941,
author = {Auer, Peter},
title = {Using Confidence Bounds for Exploitation-Exploration Trade-Offs},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {We show how a standard tool from statistics --- namely confidence bounds --- can be used to elegantly deal with situations which exhibit an exploitation-exploration trade-off. Our technique for designing and analyzing algorithms for such situations is general and can be applied when an algorithm has to make exploitation-versus-exploration decisions based on uncertain information provided by a random process. We apply our technique to two models with such an exploitation-exploration trade-off. For the adversarial bandit problem with shifting our new algorithm suffers only O((ST)1/2) regret with high probability over T trials with S shifts. Such a regret bound was previously known only in expectation. The second model we consider is associative reinforcement learning with linear value functions. For this model our technique improves the regret from O(T3/4) to O(T1/2).},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {397–422},
numpages = {26},
keywords = {reinforcement learning, online Learning, exploitation-exploration, linear value function, bandit problem}
}



@article{DBLP:journals/corr/abs-1709-05380,
  author    = {Brendan O'Donoghue and
               Ian Osband and
               R{\'{e}}mi Munos and
               Volodymyr Mnih},
  title     = {The Uncertainty Bellman Equation and Exploration},
  journal   = {CoRR},
  volume    = {abs/1709.05380},
  year      = {2017},
  url       = {http://arxiv.org/abs/1709.05380},
  eprinttype = {arXiv},
  eprint    = {1709.05380},
  timestamp = {Mon, 13 Aug 2018 16:49:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1709-05380.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1810-12162,
  author    = {Pranav Shyam and
               Wojciech Jaskowski and
               Faustino Gomez},
  title     = {Model-Based Active Exploration},
  journal   = {CoRR},
  volume    = {abs/1810.12162},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.12162},
  eprinttype = {arXiv},
  eprint    = {1810.12162},
  timestamp = {Thu, 01 Nov 2018 18:03:07 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1810-12162.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@Book{Bellman:1957,
  author =       "Bellman, Richard",
  title =        "Dynamic Programming",
  publisher =    "Princeton University Press",
  year =         "1957",
  address =   "Princeton, NJ, USA",
  edition =   "1",
  url = "http://books.google.com/books?id=fyVtp3EMxasC&pg=PR5&dq=dynamic+programming+richard+e+bellman&client=firefox-a#v=onepage&q=dynamic%20programming%20richard%20e%20bellman&f=false",
  bib2html_rescat = "General RL",
}

@book{howard:dp,
  added-at = {2008-03-11T14:52:34.000+0100},
  address = {Cambridge, MA},
  author = {Howard, R. A.},
  biburl = {https://www.bibsonomy.org/bibtex/28b55f737ee6dd7800ffc7952a33bb6bd/idsia},
  citeulike-article-id = {2380352},
  interhash = {7eed9f4f6bd1f9ee063d80d0f732e48f},
  intrahash = {8b55f737ee6dd7800ffc7952a33bb6bd},
  keywords = {inaki},
  priority = {2},
  publisher = {MIT Press},
  timestamp = {2008-03-11T14:56:12.000+0100},
  title = {Dynamic Programming and Markov Processes},
  year = 1960
}
@book{DBLP:books/lib/Bertsekas05,
  author    = {Dimitri P. Bertsekas},
  title     = {Dynamic programming and optimal control, 3rd Edition},
  publisher = {Athena Scientific},
  year      = {2005}
}

@article{10.1007/BF00992699,
author = {Lin, Long-Ji},
title = {Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching},
year = {1992},
issue_date = {May 1992},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {8},
number = {3–4},
issn = {0885-6125},
url = {https://doi.org/10.1007/BF00992699},
doi = {10.1007/BF00992699},
abstract = {To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.This paper compares eight reinforcement learning frameworks: adaptive heuristic critic (AHC) learning due to Sutton, Q-learning due to Watkins, and three extensions to both basic methods for speeding up learning. The three extensions are experience replay, learning action models for planning, and teaching. The frameworks were investigated using connectionism as an approach to generalization. To evaluate the performance of different frameworks, a dynamic environment was used as a testbed. The environment is moderately complex and nondeterministic. This paper describes these frameworks and algorithms in detail and presents empirical evaluation of the frameworks.},
journal = {Mach. Learn.},
month = {may},
pages = {293–321},
numpages = {29},
keywords = {teaching, Reinforcement learning, planning, connectionist networks}
}

@TechReport{SCC.Barto.Bradtke.ea1991,

  author      = {Barto, Andrew Gehret and Bradtke, Steven J. and Singh, Satinder P.},

  title       = {Real-time learning and control using asynchronous dynamic programming},

  institution = {University of Massachusetts at Amherst, Department of Computer and Information Science},

  year        = {1991},

  owner       = {rkamalapurkar},

  timestamp   = {2016.05.18},

}

@article{DBLP:journals/corr/abs-1301-3878,
  author    = {Andrew Y. Ng and
               Michael I. Jordan},
  title     = {{PEGASUS:} {A} Policy Search Method for Large MDPs and POMDPs},
  journal   = {CoRR},
  volume    = {abs/1301.3878},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.3878},
  eprinttype = {arXiv},
  eprint    = {1301.3878},
  timestamp = {Mon, 13 Aug 2018 16:49:02 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1301-3878.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{MAL-086,
url = {http://dx.doi.org/10.1561/2200000086},
year = {2023},
volume = {16},
journal = {Foundations and Trends® in Machine Learning},
title = {Model-based Reinforcement Learning: A Survey},
doi = {10.1561/2200000086},
issn = {1935-8237},
number = {1},
pages = {1-118},
author = {Thomas M. Moerland and Joost Broekens and Aske Plaat and Catholijn M. Jonker}
}

@article{Sutton:1990,
  added-at = {2009-06-26T15:25:19.000+0200},
  address = {San Mateo, CA},
  author = {Sutton, Richard S.},
  biburl = {https://www.bibsonomy.org/bibtex/246c73a3f5332fd7c87ea2a49b8e30255/butz},
  description = {diverse cognitive systems bib},
  interhash = {cb1bb3c7c8656dd4e0e59c4608b6f4e3},
  intrahash = {46c73a3f5332fd7c87ea2a49b8e30255},
  journal = {Proceedings of the Seventh International Conference on Machine Learning},
  keywords = {imported},
  pages = {216-224},
  publisher = {Morgan Kaufmann},
  timestamp = {2009-06-26T15:25:56.000+0200},
  title = {Integrated architectures for learning, planning, and reacting based
	on approximating dynamic programming},
  year = 1990
}

@article{10.1145/122344.122377,
author = {Sutton, Richard S.},
title = {Dyna, an Integrated Architecture for Learning, Planning, and Reacting},
year = {1991},
issue_date = {Aug. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {0163-5719},
url = {https://doi.org/10.1145/122344.122377},
doi = {10.1145/122344.122377},
abstract = {Dyna is an AI architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures.},
journal = {SIGART Bull.},
month = {jul},
pages = {160–163},
numpages = {4}
}

@article{DBLP:journals/corr/cs-AI-9605103,
  author    = {Leslie Pack Kaelbling and
               Michael L. Littman and
               Andrew W. Moore},
  title     = {Reinforcement Learning: {A} Survey},
  journal   = {CoRR},
  volume    = {cs.AI/9605103},
  year      = {1996},
  url       = {https://arxiv.org/abs/cs/9605103},
  timestamp = {Fri, 10 Jan 2020 12:58:17 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/cs-AI-9605103.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{BARTO1994888,
title = {Reinforcement learning control},
journal = {Current Opinion in Neurobiology},
volume = {4},
number = {6},
pages = {888-893},
year = {1994},
issn = {0959-4388},
doi = {https://doi.org/10.1016/0959-4388(94)90138-4},
url = {https://www.sciencedirect.com/science/article/pii/0959438894901384},
author = {Andrew G. Barto},
abstract = {Reinforcement learning refers to improving performance through trial-and-error. Despite recent progress in developing artificial learning systems, including new learning methods for artificial neural networks, most of these systems learn under the tutelage of a knowledgeable ‘teacher’ able to tell them how to respond to a set of training stimuli. Learning under these conditions is not adequate, however, when it is costly, or even impossible, to obtain this kind of training information. Reinforcement learning is attracting increasing attention in computer science and engineering because it can be used by autonomous systems to learn from their experiences instead of from knowledgeable teachers, and it is attracting attention in computational neuroscience because it is consonant with biological principles. Recent research has improved the efficiency of reinforcement learning and has provided some striking examples of its capabilities.}
}

@misc{1606.01540,
  Author = {Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
  Title = {OpenAI Gym},
  Year = {2016},
  Eprint = {arXiv:1606.01540},
}

@article{barto1990learning,
  title={Learning and sequential decision making},
  author={Barto, Andrew G and Sutton, Richard S and Watkins, Christopher JCH},
  journal={Learning and Computational Neuroscience: Foundations of Adaptive Networks},
  pages={539--602},
  year={1990},
  publisher={MIT Press}
}

@book{GhallabNauTraverso04,
  abstract = {Automated planning technology now plays a significant role in a variety of demanding applications, ranging from controlling space vehicles and robots to playing the game of bridge. These real-world applications create new opportunities for synergy between theory and practice: observing what works well in practice leads to better theories of planning, and better theories lead to better performance of practical applications. Automated Planning mirrors this dialogue by offering a comprehensive, up-to-date resource on both the theory and practice of automated planning. The book goes well beyond classical planning, to include temporal planning, resource scheduling, planning under uncertainty, and modern techniques for plan generation, such as task decomposition, propositional satisfiability, constraint satisfaction, and model checking.},
  added-at = {2017-10-14T20:24:20.000+0200},
  address = {Amsterdam},
  author = {Ghallab, Malik and Nau, Dana and Traverso, Paolo},
  biburl = {https://www.bibsonomy.org/bibtex/282d939e940c148c781f00acfc2a3d062/flint63},
  file = {ScienceDirect:2000-04/GhallabNauTraverso04.pdf:PDF;Amazon Search inside:http\://www.amazon.de/gp/reader/1558608567/:URL},
  groups = {public},
  interhash = {3d4fc1ca9626966df8dfff4e2ba3ecc7},
  intrahash = {82d939e940c148c781f00acfc2a3d062},
  isbn = {978-1-55860-856-6},
  keywords = {01801 105 book shelf elsevier ai plan knowledge processing temporal},
  publisher = {Morgan Kaufmann},
  series = {The Morgan Kaufmann Series in Artificial Intelligence},
  timestamp = {2018-04-16T12:15:22.000+0200},
  title = {Automated Planning: Theory and Practice},
  url = {http://www.sciencedirect.com/science/book/9781558608566},
  username = {flint63},
  year = 2004
}

@book{10.5555/528623,
author = {Puterman, Martin L.},
title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
year = {1994},
isbn = {0471619779},
publisher = {John Wiley &amp; Sons, Inc.},
address = {USA},
edition = {1st},
abstract = {From the Publisher:The past decade has seen considerable theoretical and applied research on Markov decision processes, as well as the growing use of these models in ecology, economics, communications engineering, and other fields where outcomes are uncertain and sequential decision-making processes are needed. A timely response to this increased activity, Martin L. Puterman's new work provides a uniquely up-to-date, unified, and rigorous treatment of the theoretical, computational, and applied research on Markov decision process models. It discusses all major research directions in the field, highlights many significant applications of Markov decision processes models, and explores numerous important topics that have previously been neglected or given cursory coverage in the literature. Markov Decision Processes focuses primarily on infinite horizon discrete time models and models with discrete time spaces while also examining models with arbitrary state spaces, finite horizon models, and continuous-time discrete state models. The book is organized around optimality criteria, using a common framework centered on the optimality (Bellman) equation for presenting results. The results are presented in a "theorem-proof" format and elaborated on through both discussion and examples, including results that are not available in any other book. A two-state Markov decision process model, presented in Chapter 3, is analyzed repeatedly throughout the book and demonstrates many results and algorithms. Markov Decision Processes covers recent research advances in such areas as countable state space models with average reward criterion, constrained models, and models with risk sensitive optimality criteria. It also explores several topics that have received little or no attention in other books, including modified policy iteration, multichain models with average reward criterion, and sensitive optimality. In addition, a Bibliographic Remarks section in each chapter comments on relevant historic}
}

@article{DBLP:journals/corr/abs-1804-07779,
  author    = {Fangkai Yang and
               Daoming Lyu and
               Bo Liu and
               Steven Gustafson},
  title     = {{PEORL:} Integrating Symbolic Planning and Hierarchical Reinforcement
               Learning for Robust Decision-Making},
  journal   = {CoRR},
  volume    = {abs/1804.07779},
  year      = {2018},
  url       = {http://arxiv.org/abs/1804.07779},
  eprinttype = {arXiv},
  eprint    = {1804.07779},
  timestamp = {Fri, 10 May 2019 11:01:58 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1804-07779.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Kuvayev1996ModelBasedRL,
  title={Model-Based Reinforcement Learning with an Approximate, Learned Model},
  author={Leonid Kuvayev and Richard S. Sutton},
  year={1996}
}

@article{SUTTON1999181,
title = {Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
journal = {Artificial Intelligence},
volume = {112},
number = {1},
pages = {181-211},
year = {1999},
issn = {0004-3702},
doi = {https://doi.org/10.1016/S0004-3702(99)00052-1},
url = {https://www.sciencedirect.com/science/article/pii/S0004370299000521},
author = {Richard S. Sutton and Doina Precup and Satinder Singh},
keywords = {Temporal abstraction, Reinforcement learning, Markov decision processes, Options, Macros, Macroactions, Subgoals, Intra-option learning, Hierarchical planning, Semi-Markov decision processes},
abstract = {Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key, longstanding challenges for AI. In this paper we consider how these challenges can be addressed within the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action in this framework to include options—closed-loop policies for taking action over a period of time. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Overall, we show that options enable temporally abstract knowledge and action to be included in the reinforcement learning framework in a natural and general way. In particular, we show that options may be used interchangeably with primitive actions in planning methods such as dynamic programming and in learning methods such as Q-learning. Formally, a set of options defined over an MDP constitutes a semi-Markov decision process (SMDP), and the theory of SMDPs provides the foundation for the theory of options. However, the most interesting issues concern the interplay between the underlying MDP and the SMDP and are thus beyond SMDP theory. We present results for three such cases: (1) we show that the results of planning with options can be used during execution to interrupt options and thereby perform even better than planned, (2) we introduce new intra-option methods that are able to learn about an option from fragments of its execution, and (3) we propose a notion of subgoal that can be used to improve the options themselves. All of these results have precursors in the existing literature; the contribution of this paper is to establish them in a simpler and more general setting with fewer changes to the existing reinforcement learning framework. In particular, we show that these results can be obtained without committing to (or ruling out) any particular approach to state abstraction, hierarchy, function approximation, or the macro-utility problem.}
}

@article{KORF1990189,
title = {Real-time heuristic search},
journal = {Artificial Intelligence},
volume = {42},
number = {2},
pages = {189-211},
year = {1990},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(90)90054-4},
url = {https://www.sciencedirect.com/science/article/pii/0004370290900544},
author = {Richard E. Korf},
abstract = {We apply the two-player game assumptions of limited search horizon and commitment to moves in constant time, to single-agent heuristic search problems. We present a variation of minimax lookahead search, and an analog to alpha-beta pruning that significantly improves the efficiency of the algorithm. Paradoxically, the search horizon reachable with this algorithm increases with increasing branching factor. In addition, we present a new algorithm, called Real-Time-A∗, for interleaving planning and execution. We prove that the algorithm makes locally optimal decisions and is guaranteed to find a solution. We also present a learning version of this algorithm that improves its performance over successive problem solving trials by learning more accurate heuristic values, and prove that the learned values converge to their exact values along every optimal path. These algorithms effectively solve significantly larger problems than have previously been solvable using heuristic evaluation functions.}
}

@ARTICLE{5391906,

  author={Samuel, A. L.},

  journal={IBM Journal of Research and Development}, 

  title={Some Studies in Machine Learning Using the Game of Checkers. II—Recent Progress}, 

  year={1967},

  volume={11},

  number={6},

  pages={601-617},

  doi={10.1147/rd.116.0601}}


@InProceedings{10.1007/978-3-540-77949-0_6,
author="Grounds, Matthew
and Kudenko, Daniel",
editor="Tuyls, Karl
and Nowe, Ann
and Guessoum, Zahia
and Kudenko, Daniel",
title="Combining Reinforcement Learning with Symbolic Planning",
booktitle="Adaptive Agents and Multi-Agent Systems III. Adaptation and Multi-Agent Learning",
year="2008",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="75--86",
isbn="978-3-540-77949-0"
}

@inCollection{RLSOTA11,
    author =    {Todd Hester and Peter Stone},
    title =     {Learning and Using Models},
    booktitle = {Reinforcement Learning: State of the Art},
    editor =    {Marco Wiering and Martijn van Otterlo},
    year =      {2011},
    address =	 {Berlin, Germany},
    publisher =	 {Springer Verlag},
abstract = "As opposed to model-free RL methods, which learn directly from 
experience in the domain, model-based methods learn a model of the transition 
and reward functions of the domain on-line and plan a policy using this model.
Once the method has learned an accurate model, it can plan 
an optimal policy on this model without any further experience in the world.
Therefore, when model-based methods are able to learn a good model quickly,
they frequently have improved sample efficiency over model-free methods, 
which must continue taking actions in the world for values to 
propagate back to previous states. 
Another advantage of model-based methods is that they can use
their models to plan multi-step exploration trajectories. In particular,
many methods drive the agent to explore where there is uncertainty in the model,
so as to learn the model as fast as possible.
In this chapter, we 
survey some of the types of models used in model-based methods and ways of learning
them, as well as methods for planning on these models.
In addition, we examine the typical architectures for
combining model learning and planning, which vary depending on whether the
designer wants the algorithm to run on-line, in batch mode, or in
real-time. 
One of the main performance criteria for these algorithms
is sample complexity, or how many actions the
algorithm must take to learn. We examine the sample efficiency of a few methods,
which are highly dependent on having intelligent exploration mechanisms. We 
survey some approaches to solving the exploration problem, including Bayesian
methods that maintain a belief distribution over possible models to explicitly
measure uncertainty in the model. 
We show some empirical comparisons of various model-based and model-free 
methods on two example domains before concluding with a survey of current
research on scaling these methods up to larger domains with
improved sample and computational complexity.",
}

@article{DBLP:journals/corr/abs-2006-15009,
  author    = {Thomas M. Moerland and
               Joost Broekens and
               Catholijn M. Jonker},
  title     = {A Framework for Reinforcement Learning and Planning},
  journal   = {CoRR},
  volume    = {abs/2006.15009},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.15009},
  eprinttype = {arXiv},
  eprint    = {2006.15009},
  timestamp = {Wed, 01 Jul 2020 15:21:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2006-15009.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{osband2020bsuite,
    title={Behaviour Suite for Reinforcement Learning},
    author={Osband, Ian and
            Doron, Yotam and
            Hessel, Matteo and
            Aslanides, John and
            Sezener, Eren and
            Saraiva, Andre and
            McKinney, Katrina and
            Lattimore, Tor and
            {Sz}epesv{\'a}ri, Csaba and
            Singh, Satinder and
            Van Roy, Benjamin and
            Sutton, Richard and
            Silver, David and
            van Hasselt, Hado},
    booktitle={International Conference on Learning Representations},
    year={2020},
    url={https://openreview.net/forum?id=rygf-kSYwH}
}

@book{series/synthesis/2010Szepesvari,
  added-at = {2010-09-04T00:00:00.000+0200},
  author = {Szepesvári, Csaba},
  biburl = {https://www.bibsonomy.org/bibtex/2a2877247f9323bac6f51f2ccc231efb5/dblp},
  booktitle = {Algorithms for Reinforcement Learning},
  ee = {http://dx.doi.org/10.2200/S00268ED1V01Y201005AIM009},
  interhash = {46c07bb11e2b5fe91663d461efbc2af1},
  intrahash = {a2877247f9323bac6f51f2ccc231efb5},
  keywords = {dblp},
  publisher = {Morgan & Claypool Publishers},
  series = {Synthesis Lectures on Artificial Intelligence and Machine Learning},
  timestamp = {2010-09-07T11:31:49.000+0200},
  title = {Algorithms for Reinforcement Learning},
  url = {http://dx.doi.org/10.2200/S00268ED1V01Y201005AIM009},
  year = 2010
}

@book{vonneumann.morgenstern47,
  added-at = {2009-03-22T18:35:23.000+0100},
  address = {New Jersey},
  author = {{von Neumann}, John and Morgenstern, Oskar},
  biburl = {https://www.bibsonomy.org/bibtex/2e1ca690426c45587ad6dc9e9e0086306/mtasdemir},
  description = {tasdemir_local},
  edition = {Second},
  interhash = {43a00d641d5aa16c23ba45ea210b15fc},
  intrahash = {e1ca690426c45587ad6dc9e9e0086306},
  keywords = {imported},
  owner = {murat},
  publisher = {Princeton University Press},
  timestamp = {2009-03-22T18:38:05.000+0100},
  title = {Theory of Games and Economic Behavior},
  year = 1947
}

@article{DBLP:journals/corr/abs-1712-01815,
  author    = {David Silver and
               Thomas Hubert and
               Julian Schrittwieser and
               Ioannis Antonoglou and
               Matthew Lai and
               Arthur Guez and
               Marc Lanctot and
               Laurent Sifre and
               Dharshan Kumaran and
               Thore Graepel and
               Timothy P. Lillicrap and
               Karen Simonyan and
               Demis Hassabis},
  title     = {Mastering Chess and Shogi by Self-Play with a General Reinforcement
               Learning Algorithm},
  journal   = {CoRR},
  volume    = {abs/1712.01815},
  year      = {2017},
  url       = {http://arxiv.org/abs/1712.01815},
  eprinttype = {arXiv},
  eprint    = {1712.01815},
  timestamp = {Mon, 13 Aug 2018 16:46:01 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1712-01815.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1911-08265,
  author    = {Julian Schrittwieser and
               Ioannis Antonoglou and
               Thomas Hubert and
               Karen Simonyan and
               Laurent Sifre and
               Simon Schmitt and
               Arthur Guez and
               Edward Lockhart and
               Demis Hassabis and
               Thore Graepel and
               Timothy P. Lillicrap and
               David Silver},
  title     = {Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
  journal   = {CoRR},
  volume    = {abs/1911.08265},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.08265},
  eprinttype = {arXiv},
  eprint    = {1911.08265},
  timestamp = {Mon, 02 Dec 2019 17:48:37 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-08265.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{krause1973taxicab,
  title={Taxicab geometry},
  author={Krause, Eugene F},
  journal={The Mathematics Teacher},
  volume={66},
  number={8},
  pages={695--706},
  year={1973},
  publisher={JSTOR}
} 

@ARTICLE{6313077,

  author={Barto, Andrew G. and Sutton, Richard S. and Anderson, Charles W.},

  journal={IEEE Transactions on Systems, Man, and Cybernetics}, 

  title={Neuronlike adaptive elements that can solve difficult learning control problems}, 

  year={1983},

  volume={SMC-13},

  number={5},

  pages={834-846},

  doi={10.1109/TSMC.1983.6313077}}


@TECHREPORT{Moore90efficientmemory-based,
    author = {Andrew William Moore},
    title = {Efficient Memory-based Learning for Robot Control},
    institution = {University of Cambridge},
    year = {1990}
}

@inproceedings{Hayamizu2021GuidingRE,
  title={Guiding Robot Exploration in Reinforcement Learning via Automated Planning},
  author={Yohei Hayamizu and S. Amiri and Kishan Chandan and Keiki Takadama and Shiqi Zhang},
  booktitle={International Conference on Automated Planning and Scheduling},
  year={2021}
}

@article{Jiang2019TaskMotionPW,
  title={Task-Motion Planning with Reinforcement Learning for Adaptable Mobile Service Robots},
  author={Yuqian Jiang and Fangkai Yang and Shiqi Zhang and Peter Stone},
  journal={2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  year={2019},
  pages={7529-7534}
}

@article{scott1996,
author = {Scott, Paul and Markovitch, Shaul},
year = {1996},
month = {09},
pages = {},
title = {Learning Novel Domains Through Curiosity and Conjecture}
}


@InProceedings{pmlr-v97-jinnai19b,
  title = 	 {Discovering Options for Exploration by Minimizing Cover Time},
  author =       {Jinnai, Yuu and Park, Jee Won and Abel, David and Konidaris, George},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3130--3139},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/jinnai19b/jinnai19b.pdf},
  url = 	 {https://proceedings.mlr.press/v97/jinnai19b.html},
  abstract = 	 {One of the main challenges in reinforcement learning is solving tasks with sparse reward. We show that the difficulty of discovering a distant rewarding state in an MDP is bounded by the expected cover time of a random walk over the graph induced by the MDP’s transition dynamics. We therefore propose to accelerate exploration by constructing options that minimize cover time. We introduce a new option discovery algorithm that diminishes the expected cover time by connecting the most distant states in the state-space graph with options. We show empirically that the proposed algorithm improves learning in several domains with sparse rewards.}
}

@inproceedings{
Jinnai2020Exploration,
title={Exploration in Reinforcement Learning with Deep Covering Options},
author={Yuu Jinnai and Jee Won Park and Marlos C. Machado and George Konidaris},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkeIyaVtwB}
}

@inproceedings{10.1145/1390156.1390288,
author = {Szita, Istv\'{a}n and L\H{o}rincz, Andr\'{a}s},
title = {The Many Faces of Optimism: A Unifying Approach},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390288},
doi = {10.1145/1390156.1390288},
abstract = {The exploration-exploitation dilemma has been an intriguing and unsolved problem within the framework of reinforcement learning. "Optimism in the face of uncertainty" and model building play central roles in advanced exploration methods. Here, we integrate several concepts and obtain a fast and simple algorithm. We show that the proposed algorithm finds a near-optimal policy in polynomial time, and give experimental evidence that it is robust and efficient compared to its ascendants.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1048–1055},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@article{10.1162/153244303765208377,
author = {Brafman, Ronen I. and Tennenholtz, Moshe},
title = {R-Max - a General Polynomial Time Algorithm for near-Optimal Reinforcement Learning},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
url = {https://doi.org/10.1162/153244303765208377},
doi = {10.1162/153244303765208377},
abstract = {R-MAX is a very simple model-based reinforcement learning algorithm which can attain near-optimal average reward in polynomial time. In R-MAX, the agent always maintains a complete, but possibly inaccurate model of its environment and acts based on the optimal policy derived from this model. The model is initialized in an optimistic fashion: all actions in all states return the maximal possible reward (hence the name). During execution, it is updated based on the agent's observations. R-MAX improves upon several previous algorithms: (1) It is simpler and more general than Kearns and Singh's E3 algorithm, covering zero-sum stochastic games. (2) It has a built-in mechanism for resolving the exploration vs. exploitation dilemma. (3) It formally justifies the ``optimism under uncertainty'' bias used in many RL algorithms. (4) It is simpler, more general, and more efficient than Brafman and Tennenholtz's LSG algorithm for learning in single controller stochastic games. (5) It generalizes the algorithm by Monderer and Tennenholtz for learning in repeated games. (6) It is the only algorithm for learning in repeated games, to date, which is provably efficient, considerably improving and simplifying previous algorithms by Banos and by Megiddo.},
journal = {J. Mach. Learn. Res.},
month = {mar},
pages = {213–231},
numpages = {19},
keywords = {Markov decision processes, stochastic games, provably efficient learning, reinforcement learning, learning in games}
}

@inproceedings{NIPS2008_d5cfead9,
 author = {Nouri, Ali and Littman, Michael},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Multi-resolution Exploration in Continuous Spaces},
 url = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf},
 volume = {21},
 year = {2008}
}

@inproceedings{Littman2011EfficientME,
  title={Efficient model-based exploration in continuous state-space environments},
  author={Michael L. Littman and Ali Nouri},
  year={2011}
}

@inproceedings{Epshteyn2008ActiveRL,
  title={Active reinforcement learning},
  author={Arkady Epshteyn and Adam Vogel and Gerald DeJong},
  booktitle={International Conference on Machine Learning},
  year={2008}
}

@article{JORDAN1992307,
title = {Forward models: Supervised learning with a distal teacher},
journal = {Cognitive Science},
volume = {16},
number = {3},
pages = {307-354},
year = {1992},
issn = {0364-0213},
doi = {https://doi.org/10.1016/0364-0213(92)90036-T},
url = {https://www.sciencedirect.com/science/article/pii/036402139290036T},
author = {Michael I. Jordan and David E. Rumelhart},
abstract = {Internal models of the environment have an important role to play in adaptive systems, in general, and are of particular importance for the supervised learning paradigm. In this article we demonstrate that certain classical problems associated with the notion of the “teacher” in supervised learning can be solved by judicious use of learned internal models as components of the adaptive system. In particular, we show how supervised learning algorithms can be utilized in cases in which an unknown dynamical system intervenes between actions and desired outcomes. Our approach applies to any supervised learning algorithm that is capable of learning in multilayer networks.}
}